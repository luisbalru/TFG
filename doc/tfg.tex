\input{preambuloSimple.tex}
\graphicspath{ {./images/} }
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{soul}
\usepackage{amssymb}


%----------------------------------------------------------------------------------------
%	TÍTULO Y DATOS DEL ALUMNO
%----------------------------------------------------------------------------------------

\title{	
	\normalfont \normalsize 
	\textsc{\textbf{Aprendizaje Automático (2019)} \\ Doble Grado en Ingeniería Informática y Matemáticas \\ Universidad de Granada} \\ [25pt] % Your university, school and/or department name(s)
	\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
	\huge Memoria Práctica 3 \\ % The assignment title
	\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Luis Balderas Ruiz \\ \texttt{luisbalderas@correo.ugr.es}} 
% Nombre y apellidos 


\date{\normalsize\today} % Incluye la fecha actual

%----------------------------------------------------------------------------------------
% DOCUMENTO
%----------------------------------------------------------------------------------------

\begin{document}
	
	\maketitle % Muestra el Título
	
	\newpage %inserta un salto de página
	
	\tableofcontents % para generar el índice de contenidos
	
	\listoffigures
	
	\listoftables
	
	\newpage




\section{Contexto: Parkinson y Definición del Problema}

En la actualidad, las enfermedades neurodegenerativas son una de las afecciones más preocupantes para el ser humano y, como tal, es uno de los campos de investigación más importantes que existen. Según \textit{Parkinson's Foundation \cite{pf}}, 46.8 millones de personas en todo el mundo conviven con algún tipo de demencia. Estudios anteriores preveían que, en el año 2020, 42.3 millones de ciudadanos estarían afectados por estas enfermedades. Sin embargo, la ratio de enfermos se ha superado en más de 4 millones un año antes de la fecha esperada, lo que genera una preocupación acuciante. El mismo estudio pronosticó que el número de pacientes con demencia se duplicará en los próximos 20 años. \\

La demencia, a grandes rasgos, es un estado caracterizado por el deterioro de las funciones cerebrales. Este deterioro o pérdida de facultades da lugar a grandes inconvenientes en el día a día, llegando a extremos tan graves como la pérdida de la consciencia. Se estima que hay más de 10 millones de personas enfermos de Parkinson (en lo que sigue, PD) alrededor del mundo (\cite{wp}). Este hecho hace que la investigación de esta enfermedad en concreto sea muy relevante, dado que un diagnóstico precoz podría frenar el desarrollo de la misma. Desgraciadamente, actualmente no existe una cura para el Parkinson, pero sí hay medicamentos que inhiben su desarrollo, dándoles a los pacientes un mínimo de calidad de vida durante un periodo de tiempo más extenso. \\

Los principales métodos de diagnóstico se fundamentan en resultados clínicos, basados en la evaluación médica a través de distintas pruebas al paciente. El diagnóstico actual recae en la presencia de anomalías o disfunciones motoras, signo de que el paciente sufre indudablemente un PD en estado avanzado. En dicho estado, la terapia neuroprotectora apenas produce mejorías sustanciales en los pacientes, por lo que es verdaderamente importante encontrar biomarcadores objetivos y válidos que ayuden a distinguir entre pacientes enfermos de PD de la población sana. \\

En las dos décadas anteriores se adoptaron diversas medidas para el diagnóstico diferencial de PD, incluyendo pruebas olfativas, electrofisiológicas y neuropsicológicas \cite{pruebas-ant}. Sin embargo, neuroimagen es el área más desarrollada para enfrentar diagnósticos. Estos métodos incluye la Imagen de Resonancia Magnética (MRI). MRI es una tecnología no invasiva con una gran resolución espacio-temporal y ha sido enormemente utilizado para el estudio de disfunciones cerebrales de todo tipo. La gran cantidad de información que MRI nos da sobre los tejidos ha mejorado de forma muy sustancial el diagnóstico de patologías cerebrales y su tratamiento. Es conveniente señalar que la basta cantidad de información que nos da está lejos de poder ser procesada manualmente, por lo que urge el desarrollo de herramientas de análisis automatizado. Dicha necesidad hace nacer este proyecto, basado en la extracción de características de imágenes cerebrales para la clasificación de sujetos en enfermos de PD o grupo de control con la mayor exactitud posible.

\subsection{Objetivos del proyecto}

El objetivo principal de este proyecto es diseñar y desarrollar un sistema avanzado que clasifique pacientes en enfermos y sanos tras analizar y refinar datos provenientes de MRI, así como descubrir qué zonas del cerebro son las más determinantes en el diagnóstico de la enfermedad. \\

Existen multitud de artículos en la literatura que llevan a cabo clasificación de pacientes enfermos y sanos. Dicha clasificación de enfermos de Alzheimer o Parkinson suelen estar basadas en la evaluación de las capacidades motoras de los individuos. Sin embargo, este enfoque sobre Parkinson utilizando MRI es novedoso. De la misma manera, utilizaremos métodos de extracción y selección de características, en particular transformada Wavelets 2D y PCA (Análisis de Componentes Principales). Este enfoque ha sido previamente sugerido por otros investigadores (\cite{aggarwal}, \cite{iman}, \cite{deepa}, \cite{mohd}, \cite{rajesh}, \cite{michel}, \cite{jing}, \cite{yudong}, \cite{irojas}, \cite{alberto}). \\

Finalmente, la mayoría de las investigaciones exploran las regiones identificadas por expertos médicos. En este proyecto, nuestro interés es encontrar los planos más relevantes para la clasificación de enfermos de PD. Para ello, existen muchas técnicas de optimización disponible: Optimización por Colonia de Hormigas, Algoritmo de Búsqueda Gravitacional, algoritmo genético NSGA-II. En mi caso, enfoco el problema de una manera totalmente diferente utilizando un ensemble learner basado en Stacking, donde en las primeras capas utilizo SVM con GridSearch para configurar los hiperparámetros y en la segunda, regresión logística (en búsqueda de interpretabilidad).

\subsection{Experimentos}

En la realización del proyecto utilizamos diferentes algoritmos y métodos, incluyendo preprocesamiento de imágenes, extracción, selección de características, clasificación y optimización de los resultados. Los experimentos designados son los siguientes:

\begin{itemize}
	\item El primer experimento fue determinar qué plano de una imagen MRI es más importante en la clasificación de los sujetos. Las imágenes MRI tienen tres planes: X (axial), Y (coronal) y Z (sagittal). Para reducir el tiempo computacional, seleccionamos el plano con los cortes más interesantes.
	
	\item En el segundo experimento, elegido ya el plano correspondiente, comparo el rendimiento de la materia gris, materia blanca y el materia completa para ahorrar costes y mejorar la clasificación.
	
	\item Para la extracción de características, utilizo la transformada discreta Wavelet en 2D.
	
\end{itemize}


\section{Enfermedad de Parkinson y sus estados}

\subsection{Contexto global}

\textit{Parkinson's Foundation} (\cite{pf}) describe el Parkison como sigue: \\

`` El Parkinson es un desorden neurodegenerativo que causa la muerte de las neuronas dopaminérgicas (neurotrasmisores que producen y secretan dopamina) de un área concreta del cerebro llamada \textit{substantia nigra pars compacta (SNpc)}.''

Esta estructura se encuentra localizada en el mesencéfalo y debe su color y su nombre ala presencia de un pigmento llamado neuromelanina que se encuentra dentro de las neuronas que lo forman. 

\begin{figure}[H]
	\begin{minipage}[b]{0.5\linewidth}
		\centering
		\includegraphics[width=\linewidth]{sub-n.png}
		\caption{Susbtantia nigra}
		\label{fig:subs-nig}
	\end{minipage}
	\hspace{0.5cm}
	\begin{minipage}[b]{0.5\linewidth}
		\centering
		\includegraphics[width=\linewidth]{mesenc.png}
		\caption{Mesencéfalo}
		\label{fig:mesenc}
	\end{minipage}
\end{figure}



Estas neuronas dopaminérgicas tienen principalmente la función de regular la actividad motora por medio de la síntesis y la secreción de dopamina, por lo que cuando mueren se manifiestan los típicos signos de la enfermedad que nos resultan familiares: temblores, lentitud en el movimiento (bradiquinesia), inestabilidad, caídas frecuentes... A nivel macroscópico esto se manifiesta en la pérdida de pigmentación característica de la SNpc.

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.4]{pigme.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Pérdida de la pigmentación} 
	\label{fig:pigmentacion}
\end{figure}

Cabe destacar que en las neuronas supervivientes, a nivel microscópico se observan los característicos cuerpos de Lewy, que son unas ``bolsitas'' de proteínas que se acumulan en el citoplasma o cuerpo de la célula.

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.3]{cito.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Cuerpos de Lewy en el citoplasma} 
	\label{fig:lewy}
\end{figure}

\subsection{Corteza cerebral y ganglio basal}

La primera capa que nos encontramos al explorar el cerebro humano es la materia dura, esto es, una membrana que envuelve al cerebro, siendo la última capa de las meninges, cubriendo y por tanto protegiendo el cerebro y la médula espinal. Bajo esta membrana encontramos el cortex, formado por millones de neuronas de un color gris claro (la materia gris) organizadas en seis capas de entre dos y cuatro milímetros de grosor. La corteza cerebral juega un papel trascendental en la conciencia, el pensamiento, el lenguaje, la memoria, la percepción y la atención. La materia gris es una componete muy importante de nuestro sistema nervioso central. Por otra parte, la matria blanca está formada por axones que interconectan las neuronas en diferentes regiones de la corteza y del sistema nervioso central. \\

La corteza cerebral se divide en cuatro lóbulos: \\

\begin{itemize}
	\item Lóbulo temporal: Clave en la percepción auditiva, comprensión del lenguaje, memoria y aprendizaje. Contiene el hipocampo.
	\item Lóbulo frontal: Corteza motora primaria, contiene también la mayoría de las neuronas dopaminérgicas en el cortex.
	\item Lóbulo pariental: esencial para la visión espacial, la navegación y el sentido del tacto.
	\item Lóbulo occipital: Cortex viaul primario, responsable de la creación de los sueños.
\end{itemize}

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.6]{bl.jpg}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Lóbulos de la corteza cerebral} 
	\label{fig:bl}
\end{figure}

Las neuronas dopaminérgicas de la SNpc proyectan sus axones hacia el ganglio basal, formando así el sistema dopaminérgico nigroestriatal. El ganglio basal, que se encuentra en estrecha relación con la SNpc, está descrito como la estructura cerebral más afectada por PD. Cumple un papel esencial tanto en la ejecución de movimientos voluntarios como en actividades cognitivas, por lo que su deterior asociado a PD afectará a estas funciones. \\

El ganglio basal puede verse afectado según el subtipo de la enfermedad. Hay algunos enfermos que sufren cambios microestructurales en la substantia nigra mientras que en otros apenas se aprecia. En general, el ganglio basal acaba por atrofiarse. 

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.6]{gb.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Ganglio basal} 
	\label{fig:gb}
\end{figure}

Según \cite{wp}, los pacientes de PD muestran anisotropía fraccional reducida en la substantia nigra y aumento de la difusividad media y radial en la substantia nigra y el globo pálido (parte del ganglio basal), cuyos efectos pueden verse en técnicas de imagen tales como tractografías.

\subsection{Estadios del Parkinson}

La enfermedad de Parkinson afecta al ser humano de muy distintas maneras. Los enfermos no tienen por qué sufrir los mismos síntomas y, si lo hicieran, tampoco tienen por qué experimentarlos en el mismo orden ni con la misma intensidad. Sin embargo, existen algunos patrones típicos en el progreso de la enfermedad divididos en estadios \cite{wp}:

\begin{itemize}
	\item Estadio uno: Durante este estado inicial, la persona tiene síntomas menores que no interfieren en su vida diaria. Pueden darse temblores y movimientos involuntarios en un lado del cuerpo. De igual manera, se producen cambios posturales, en la forma de andar y en la expresión facial.
	\item Estadio dos: Los síntomas empeoran. Aparecen temblores, rigidez y movimientos involuntarios en ambos lados del cuerpo.
	\item Estadio tres: Considerado el estadio medio, se caracteriza por la ralentización de los movimientos y la pérdida del equilibrio. Las caídas empiezan a ser comunes.
	\item Estadio cuatro: En este punto, los síntomas son severos. Es posible permanecer de pie sin ayuda, pero en general se necesita un andador para desplazarse. La persona es incapaz de vivir sola y requiere asistencia.
	\item Estadio cinco: Este es el estadio más avanzado. Es imposible andar o ponerse de pie por la debilidad en las piernas. La persona requiere silla de ruedas y asistencia total para todas las actividades.
\end{itemize}

\subsection{Consecuencias en el cerebro}

Como hemos comentado, PD afecta a la substantia nigra. También reduce diferentes regiones de la materia gris en el lóbulo temporal. En la figura 2.7 podemos ver una resonancia magnética de un sujeto sano mientras que en la figura 2.8 vemos a un enfermo. Se puede observar la reducción de la materia gris en cada plano  e incluso su desaparición en algunas zonas.

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.35]{healthy.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{MRI de un sujeto sano} 
	\label{fig:healthy}
\end{figure}

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.34]{unhealthy.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{MRI de un sujeto enfermo de PD. Véase la pérdida de materia gris} 
	\label{fig:unhealthy}
\end{figure}

\section{Conceptos y herramientas matemáticas}

En la presente sección discutimos los diferentes conceptos y herramientas utilizadas en este proyecto. Más allá de utilizar la multitud de técnicas que existen dentro de la ciencia de datos aplicada a la biomedicina, mi intención es profundizar desde un punto de visto matemático sus fundamentos y las garantías de su uso. Primero, introduzco la imagen de resonancia magnética (MRI) y la normalización aplicada a las imágenes para los distintos experimentos. A continuación, trataré el concepto de característica en ciencia de datos y me detendré en las herramientas utilizadas para la extracción (transformada discreta Wavelets 2D) y selección (PCA) de las mismas. Por último, abordamos el tema de la clasificación, explicando el algoritmo SVM y las optimizaciones propuestas para la mejora del rendimiento,  interpretabilidad y validez de la investigación.

\subsection{Resonancia magnética nuclear}

Antes de trabajar con imágenes MRI, es necesario tener conocimiento sobre cómo se obtienen, así que introducimos los conceptos más importantes de la resonancia magnética nuclear (NMR \cite{nmr}). Se trata de un método de imagen no invasivo que tiene como objetivo obtener información sobre la estructura y composición de un material. Es muy utilizado en medicina para recabar información sobre tejidos y observar alteraciones o degradaciones de los mismos. \\

NMR es un fenómeno físico basado en las propiedades cuántico-mecánicas del núcleo atómico. Primero, el \textit{spin} (momento angular intrínseco \cite{spin}) de una partícula es un vector asociado al momento magnético de la misma. Tiene una dirección (el eje del \textit{spin}) y un sentido. Cuando dos o más partículas tienen \textit{spin} opuestos están pareadas, la suma de sus momentos es cero y, por tanto, no se produce manifestación alguna del \textit{spin}. Este es el estado natural de los momentos magnéticos en el cuerpo ya que el núcleo de los átomos y sus respectivos electrones están pareados. Sin embargo, podemos encontrar en el cuerpo isótopos con \textit{spin} distinto de cero, siendo los más comunes los de hidrógeno ya que la mayoría de los tejidos contienen agua. La manipulación del \textit{spin} es lo que permite a la máquina de resonancia magnética encontrar las diferencias entre las orientaciones y construir una imagen. Usando campos magnéticos, los núcleos de hidrógeno se alinean magnéticamente, produciéndose este cambio en las alineaciones en un tiempo T1. En presencia de un campo magnético externo, existen dos tipos de orientaciones para el \textit{spin} nuclear:

\begin{itemize}
	\item Paralela, en la que el sentido del momento magnético es el mismo para la partícula y para el campo magnético externo.
	\item Anti-paralelo, cuando ambos dos tienen sentido contrario.
\end{itemize}

Como se muestra en la Figura 3.1, la suma de los momentos magnéticos de un grupo de núcleos de hidrógeno puede ser representado como un vector M paralelo al campo magnético externo $B_0$, donde las componentes normales de los diferentes \textit{spin} se cancelan mutuamente. La mayoría de los \textit{spin} adoptan la orientación paralela, por lo que la suma de los \textit{spins} de M es paralela a $B_0$. \\

M puede ser manipulado usando señales de radio frecuencia. Cuando se aplican, los núcleos absorben la energía y una porción de ella se emite más tarde. Esa es la señal que se detecta. Dependiendo de la intensidad de los campos magnéticos, se pueden obtener imagénes con distintas resoluciones.

 \begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
 	\centering
 	\includegraphics[scale=0.34]{nmr.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
 	\caption{Funcionamiento de los campos magnéticos, spin y NMR} 
 	\label{fig:nmr}
 \end{figure}

\subsection{Normalización}

Antes de manipular o extraer información, es fundamental normalizar el conjunto de datos completo para poder comparar las imágenes. Cada imagen MRI debería representar el mismo espacio, i.e., cada región de la imagen debe describir la misma región del cerebro. Los ventrículos cerebrales deben estar situados en las mismas coordenadas para cada imagen de cada paciente. \\

Dado que los cerebros pueden variar mucho entre pacientes, este objetivo es difícil de conseguir ya que las herencia genética y la vida de cada paciente haga que cada uno tenga sus peculiaridades. Por tanto, debemos tener en cuenta dos ideas a cumplir:

\begin{itemize}
	\item Normalizar la escala de grises de las imágenes para que todas estén en la misma escala.
	\item Redimensionar las imágenes para que todas tengan las mismas medidas.
\end{itemize}

Para completar estas dos tareas, nos remitimos a una plantilla estándar. Para la normalización, utilizamos Statistical Parametric Mapping (SPM \cite{spm}), que es un algoritmo que analiza cada \textit{voxel} usando un test estadístico estándar. Lo utilizamos dada su gran efectividad y su librería de Matlab (a pesar de que el resto del proyecto se realiza en Python). Los valores de los \textit{voxel} son distribuidos de acuerdo a una función de densidad, que suele provenir de las distribuciones T de Student o F. En la Figura 3.2 podemos ver un ejemplo de una imagen antes y después de la normalización.

 \begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.6]{norm.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{A la izquierda, imagen MRI sin normalizar. A la derecha, resultado de la normalización} 
	\label{fig:norm}
\end{figure}

\subsection{Características}

Una de las partes más importantes de un proyecto de inteligencia artificial es la extracción de características. Una característica puede ser entendida como una componente de un vector, que representaría los datos. Cada elemento o individuo del dataset sería representado por un vector n-dimensional (una matriz) en la que cada componente sería una característica. Esta forma de representación es muy intuitiva y flexible ya que permite la introducción y aplicación de conceptos y herramientas matemáticas. \\

El dataset completo sería definido, una vez que cada elemento está representado por sus características, como un conjunto de vectores de $m$ componentes

$$F_i = [f_1,f_2,\dots,f_m]$$

En la expresión, $F_i$ sería un único vector (un paciente) y $f_i$ sus características. Por tanto, si tenemos un grupo de pacientes:

$$F =  \begin{pmatrix}
F_{1} \\
F_{2} \\
\vdots \\
F_{n}
\end{pmatrix} = \begin{pmatrix}
f_{11}& f_{12}& \dots& f_{1m} \\
f_{21}&f_{22}&\dots&f_{2m} \\
\vdots& \vdots& \ddots& \vdots \\
f_{n1}&f_{n2}&\dots&f_{nm}
\end{pmatrix}$$

\subsubsection{Extracción de características}

Cuando se trabaja con MRI, se tiene un número de imágenes de $157\times189\times136$ en los planos X,Y y Z respectivamente, por lo que tenemos 157*189*136 = 4.035.528 \textit{voxels} con 255 valores posibles. De esta manera encontraríamos más de mil millones de características. Necesitamos, por tanto, reducir el espacio de posibilidades, es decir, disminuir la cantidad de recursos necesaria para describir nuestro conjunto de datos. \\

En general, una buena estrategia para extraer características de una imagen es convertirla a un dominio diferente. Con ese objetivo, introducimos ahora dos opciones distintas: la transformada de Fourier y la transformada Wavelets (que finalmente utilizaremos). Todo el material aquí recogido proviene de multitud de textos. Son los siguientes: \cite{math-image}, \cite{canada}, \cite{daube}, \cite{castro}, \cite{misiti}.

\begin{itemize}
	\item \textbf{La Transformada de Fourier en $L^1(\mathbb{R}^d)$}
	
	Dado que la transformada de Fourier es, de forma natural, una función que toma valores en los complejos, asumimos que
	$$u: \mathbb{R}^d \mapsto \mathbb{C}$$
	
	\textbf{Definición 1.} Sea $u \in L^1(\mathbb{R}^d)$ y $\xi \in \mathbb{R}^d$. Se define la transformada de Fourier de $u$ en $\xi$ como 
	
	$$(F u)(\zeta) = \hat{u}(\xi) = \frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^d} u(x) e^{-ix\xi}dx$$


Además, podemos introducir uno de los teoremas más importantes de la transformada de Fourier: el teorema de Convolución: 

\textbf{Teorema de Convolución.} Para $u, v \in L^1(\mathbb{R}^d)$,

$$ F(u*v) = (2\pi)^{d/2}F(u)F(v)$$

\textbf{Demostración.}


Aplicando el teorema de Fubini, obtenemos

$$F(u*v)(\xi) = \frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} u(y) v(x-y) dy\text{ } e^{-ix\xi} dx = $$
$$ == \frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} u(y)e^{-iy\xi}v(x-y)e^{-i(x-y)\xi}dxdy$$
$$ = \int_{\mathbb{R}^d} u(y) e^{-iy\xi} dy F(v)(\xi)$$
$$ = (2\pi)^{d/2}F(u)F(v)(\xi)$$

\hfill$\blacksquare$

A pesar de este resultado, nos será de gran utilidad extender el concepto de transformada de Fourier al espacio de Hilbert $L^2(\mathbb{R}^d)$.

\item \textbf{La Transformada de Fourier en $L^2(\mathbb{R}^d)$}

La extensión de la transformada de Fourier al espacio $L^2(\mathbb{R}^d)$ requiere un poco más de esfuerzo. En primer lugar, definimos un ``pequeño'' espacio de funciones donde la transformada exhibe ciertas propiedades interesantes: el espacio de Schwartz:

\textbf{Definición 2.} El espacio de Schwartz se define como 

$$S(\mathbb{R}^d) = \{u \in C^\infty(\mathbb{R}) | \forall \alpha,\beta \in \mathbb{N}^d: C_{\alpha,\beta}(u) = sup_{x \in \mathbb{R}^d} |x^\alpha \frac{\partial^\beta}{\partial x^\beta}u(x)| < \infty \}$$

Una función $u \in S(\mathbb{R}^d)$ se le llama una función de Schwartz.\\

\textit{Grosso modo}, el espacio de Schwartz contiene funciones suaves que tienden a cero más rápido que los polinomios a infinito. Puede verificarse de forma elemental que el espacio de Schwartz es un espacio vectorial. Con el objetivo de hacerlo accesible por métodos analíticos, lo dotamos de una topología. Describimos dicha topología definiendo una noción de \\convergencia para sucesiones de funciones.

\textbf{Definición 3.} Una sucesión de funciones de Schwartz ${u_n}$ converge a $u$ si y sólo si, para todo multi-índice $\alpha, \beta$, se tiene que 

$$C_{\alpha,\beta}(u_n - u) \mapsto 0 \text{    cuando } n \mapsto \infty$$

La convergencia en el espacio de Schwartz es muy restrictiva: una sucesión de funciones converge si ella y todas sus derivadas multiplicadas por monomios arbitrarios convergen uniformemente. 

Los siguientes lemas serán necesarios en el desarrollo del tema:

\textbf{Lema 1.} El espacio de Schwartz es no vacío y cerrado con respecto a la derivación de cualquier orden y la multiplicación usual.

\textbf{Demostración.} Para $u \in S(\mathbb{R}^d)$, para todo multi-índice $\gamma$, tenemos 

$$C_{\alpha,\beta}(\frac{\partial^\gamma}{\partial x^\gamma}u) = C_{\alpha,\beta+\gamma}(u) < \infty$$

y por tanto $\frac{\partial^\gamma}{\partial x^\gamma}u \in S(\mathbb{R}^d)$. \\

El hecho de que dadas $u, v \in S(\mathbb{R}^d)$, $uv \in S(\mathbb{R}^d)$ puede probarse vía la regla de Leibniz para multi-índices.

\hfill$\blacksquare$

El espacio de Schwartz está muy relacionado con la transformada de Fourier. El siguiente lema presenta reglas de cálculo para transformadas de Fourier sobre funciones de Schwartz.

\textbf{Lema 2.} Sea $u \in S(\mathbb{R}^d), \alpha \in \mathbb{N}^d$ un multi-índice. Se define $p^\alpha(x) = x^\alpha$. Entonces

$$F(\frac{\partial^\alpha u}{\partial x^\alpha}) = i^{|\alpha|}p^\alpha F(u)$$

$$F(p^\alpha u) = i^{|\alpha|} \frac{\partial^\alpha }{\partial x^\alpha} F(u)$$

\textbf{Demostración.}
	Comenzamos con los siguientes cálculos auxiliares:
	
	$$\frac{\partial^\alpha }{\partial x^\alpha} (e^{-ix\xi}) = (-i)^{|\alpha|} \xi^\alpha e^{-ix\xi}$$
	
	$$x^\alpha e{ix\xi} = i^{|\alpha|}\frac{\partial^\alpha}{\partial \xi^\alpha}(e^{-ix\xi})$$
	
	Aplicando integración por partes, obtenemos
	
	$$F(\frac{\partial^\alpha}{\partial x^\alpha}u)(\xi) = \frac{1}{(2\pi)^{d/2}}\int_{\mathbb{R}^d} \frac{\partial^\alpha}{\partial x^\alpha} u(x) e^{-ix\xi}dx$$
	$$= \frac{1}{(2\pi)^{d/2}} i^{|\alpha|}\xi^\alpha \int_{\mathbb{R}^d} u(x) e^{-ix\xi}dx$$
	$$=i^{|\alpha|} p^\alpha Fu(\xi)$$
	
Intercambiando el orden de la integración y la derivación, llegamos a

$$F(p^\alpha u)(\xi) = \frac{1}{(2\pi)^{d/2}}\int_{\mathbb{R}^d} u(x)x^\alpha e^{-ix\xi}dx$$
$$=\frac{1}{(2\pi)^{d/2}}\int_{\mathbb{R}^d} u(x)\frac{\partial^\alpha}{\partial \xi^\alpha}e^{-ix\xi}dx$$
$$=i^{|\alpha|}(\frac{\partial^\alpha}{\partial \xi^\alpha}Fu)(\xi)$$

Los argumentos anteriores son válidos dado que los integrandos son infinitamente derivables respecto a $\xi$ e integrables respecto a $x$.

\hfill$\blacksquare$

Observamos que la transformada de Fourier lleva diferenciación a multiplicación y viceversa. Esto nos lleva a que el espacio de Schwartz va a sí mismo vía la transformada de Fourier. Veámoslo:\\

\textbf{Lema 3.} Para la función $G(x) = e^{-\frac{|x|^2}{2}}$, se tiene que 

$$\hat{G}(\xi) = G(\xi)$$

es decir, la función Gaussiana es una función propia de la transformada de Fourier correspondiente al valor propio uno.

\textbf{Demostración}

La función Gaussiana se puede escribir como un producto tensorial de Gaussianas unidimensionales $g:\mathbb{R} \mapsto \mathbb{R}, g(t) = exp(-t^2/2)$ de forma que $G(x) = \prod_{k=1}^{d} g(x_k)$. Por el teorema de Fubini,

$$\hat{G}(\xi) = \frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^d} \prod_{k=1}^{d} g(x_k) e^{-i x_k \xi_k} dx = \prod_{k=1}^{d} \hat{g}(\xi_k)$$

Para calcular la transformada de Fourier de $g$, tengamos en cuenta que $g$ satisface la ecuación diferencial $g'(t) = -t g(t)$. Aplicando la transformada de Fourier en esa ecuación, por el Lema 2 obtenemos $-\omega \hat{g}(\omega) = \hat{g}'(\omega)$. De hecho, $\hat{g}(0) = \int_{\mathbb{R}}g(t) dt = 1 = g(0)$. Por tanto, las funciones $g,\hat{g}$ satisfacen la misma ecuación diferencial con el mismo valor inicial. Por el teorema de unicidad de soluciones en problemas de valores iniciales de Picard-Lindelöf, $g = \hat{g}$.

\hfill$\blacksquare$

\textbf{Teorema 2.} La transformada de Fourier es una aplicación continua y biyectiva del espacio de Schwartz en sí mismo. Para $u \in S(\mathbb{R}^d)$, tenemos la fórmula de inversión

	$$(F^{-1}Fu)(x) = \overline{\hat{u}}(x)= \frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^d} \hat{u}(\xi) e^{ix\xi} d\xi = u(x)$$

\textbf{Demostración.} Por el lema 2, tenemos que para todo $\xi \in \mathbb{R}^d$

$$|\xi^\alpha \frac{\partial^\beta}{\partial \xi^\beta}\hat{u}(\xi)| = |F(\frac{\partial^\alpha}{\partial x^\alpha}p^\beta u)(\xi)| \leq \frac{1}{(2\pi)^{d/2}} \left\Vert \frac{\partial^\alpha}{\partial x^\alpha}p^\beta u \right \Vert_1  (*)$$

Por tanto, para $u \in S(\mathbb{R}^d)$, tenemos que $\hat{u} \in S(\mathbb{R}^d)$. Dado que la transformada de Fourier es lineal, es suficiente demostrar la continuidad en el cero. En efecto, consideramos a sucesión nula ${u_n}$ en el espacio de Schwartz, es decir, cuando $n \mapsto \infty, C_{\alpha,\beta}(u_n) \mapsto 0$. Esto es, ${u_n}$, al igual que ${\partial^\alpha p^\beta u_n}, \forall \alpha, \beta$, converge uniformemente a cero. Esto implica que la parte derecha de $(*)$ tiende a cero. En particular, obtenemos que $C_{\alpha,\beta}(\hat(u_n)) \mapsto 0$, lo que implica que ${\hat{u_n}}$ es una sucesión nula, probando la continuidad. \\

Para probar la fórmula de inversión, consideramos dos funciones arbitrarias $u, \phi \in S(\mathbb{R}^d)$

$$(\hat{\hat{u}}*\phi)(x) = \int_{\mathbb{R}^d} \hat{\hat{u}}(y) \phi(x-y) dy = \int_{\mathbb{R}^d}\hat{u}(y) e^{ixy} \hat{\phi}(-y) dy$$
$$ = \int_{\mathbb{R}^d} u(y) \hat{\hat{\phi}}(-x-y)dy = (u*\hat{\hat{\phi}})(-x)$$

Ahora elegimos $\phi$ para que sea una función Gaussiana reescalada:

$$\phi_{\epsilon}(x) = \epsilon^{-d}(D_{\epsilon^{-1}id}G)(x) = \epsilon^{-d}e^{-\frac{|x|^2}{2\epsilon^2}}$$

De estos cálculos inferimos que $\hat{\phi}_\epsilon = D_{\epsilon\text{ } id}\hat{G}$ y por tanto $\hat{\hat{\phi}}_\epsilon = \epsilon^{-d} D_{\epsilon^{-1}\text{ } id}\hat{\hat{G}}$. Por el lema 3, $\hat{G} = G$ y $\hat{\hat{\phi_{\epsilon}}} = \phi_{\epsilon}$. Dado que $u$ es en particular acotada y continua, y G es positiva con integral normalizada a uno, podemos aplicar las propiedades de la convolución y obtener que cuando $\epsilon \mapsto 0$,

$$\hat{\hat{u}}*\phi_{\epsilon}(x) \mapsto \hat{\hat{u}}(x)  \text{ y } u*\phi_{\epsilon}(-x) \mapsto u(-x) \Rightarrow \hat{\hat{u}}(x)=u(-x) $$

Por propiedades de conjugación de la transformada de Fourier, tenemos que $\overline{u} = D_{-id} \hat{u}$ y sustituyendo $\hat{u}$ por $u$, obtenemos:

$$\overline{\hat{u}} = D_{-id} \hat{\hat{u}} = u$$


\hfill$\blacksquare$

\textbf{Teorema 3.} Hay un único operador $F:L^2(\mathbb{R}^d) \mapsto L^2(\mathbb{R}^d)$ que extiende la transformada de Fourier $F$ a $S(\mathbb{R}^d)$ y satisface la ecuación $\left\Vert u \right \Vert_2 = \left\Vert Fu \right \Vert_2 \forall u \in L^2(\mathbb{R}^d)$. Además, $F$ es biyectivo y su inversa $F^{-1}$ es una extensión continua de $F^{-1}$ en $S(\mathbb{R}^d)$

\textbf{Demostración.} Para $u,v \in S(\mathbb{R}^d)$, sabemos que 

$$(\hat{u},\hat{v})_2 = (u,v)_2$$

y en particular $\left \Vert u \right \Vert_2 = \left \Vert Fu \right \Vert_2$. Por tanto, la transformada de Fourier es una isometría definida en un subconjunto  denso de $L^2(\mathbb{R}^d)$. Existe una única extensión continua en todo el espacio. Debido a la simetría entre $F$ y $F^{-1}$, un argumento análogo demuestra el recíproco.

\hfill$\blacksquare$

La propiedad de isometría $\left \Vert u \right \Vert_2 = \left \Vert Fu \right \Vert_2$ también implica que

$$(u,v)_2 = (Fu,Fv)_2$$
que se conoce como la fórmula de Plancherel.

\item \textbf{Transformada Wavelet}

En la sección anterior tratamos los fundamentos teóricos que sostienen la transformada de Fourier como herramienta para estudiar la representación de la frecuencia de una señal o imagen. Sin embargo, la información relacionada con la localización no es codificada de una forma plausible. En particular, una alteración local de una señal o una imagen da lugar a una modificación global de toda la transformada de Fourier. En otras palabras, la transformada de Fourier es una transformación global en el sentido de que $\hat{u(\xi)}$ depende de todos los valores de $u$. En ciertas circunstancias, las transformaciones locales son deseables. Antes de introducir nuestra herramienta fundamental en la extracción de características, la transformada Wavelet, presento una nueva alternativa para estudiar información ``local'' en la frecuencia: la transformada de Fourier de tiempo reducido o transformada ``ventana'' de Fourier (\cite{stft}):

\textbf{Definición 4.} Sean $u,g \in L^2(\mathbb{R}^d)$. La transformada de Fourier de tiempo reducido de $u$ con una función ventana $g$ se define como

$$(G_g u)(\xi,t) = \frac{1}{(2\pi)^{d/2}}\int_{\mathbb{R}^d} u(x) \overline{g(x-t)} e^{-ix\xi}dx$$

Esta transformada depende de un parámetro de frecuencia $\xi$ y de un parámetro espacial $t$ y existen muchas formas de representarla:

$$(G_g u)(\xi,t) = F(uT_{-t} \overline{g})(\xi)$$
$$ = \frac{1}{(2\pi)^{d/2}} (u, M_\xi T_{-t}g)_2$$
$$ =  \frac{1}{(2\pi)^{d/2}} (M_{-\xi}u*D_{-id}\overline{g})(t)$$

La primera alternativa explica el nombre de ``ventana'': a través de la multiplicación por la función $g$, $u$ es localizada previa la transformada de Fourier. Nótese que la transformada ``ventana'' de Fourier es una función de $2d$ variables: $(G_g u): \mathbb{R}^{2d} \mapsto \mathbb{C}$. \\

\textbf{Lema 4.} Sean $u,v,g \in L^2(\mathbb{R}^{2d})$. Entonces  $(G_g u) \in L^2(\mathbb{R}^{2d})$ y 

$$(G_g u, G_g v)_{L^2(\mathbb{R}^{2d})} = \Vert g \Vert_2^2(u,v)_2$$

\textbf{Demostración.} Para probar la igualdad entre productos escalares, usamos la isometría de la transformada de Fourier y la fórmula de Plancherel. Con $F_t$ denotamos la transformada de Fourier respecto de $t$, utilizamos una de las alternativas de representación de la transformada ventana de Fourier y el teorema de convolución para obtener

$$F_t(G_g u(\xi,·))(\omega) = F_t((2\pi)^{d/2}(M_{-\xi}u*D_{-id}\overline{g}))(\omega)$$
$$= F(M_{-\xi}u)(\omega)(F D_{-id}\overline{g})(\omega)$$
$$= \hat{u}(\omega+\xi)\overline{\hat{g}}(\omega)$$

Obtenemos el resultado con el siguiente cálculo:

$$(G_g u,G_g v)_{L^2(\mathbb{R}^{2d})} = (F_t(G_g u), F_t(G_g v))_{L^2(\mathbb{R}^{2d})}$$
$$ = \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \hat{u}(\omega + \xi) \overline{\hat{g}}(\omega) \overline{\hat{v}}(\omega+\xi)\hat{g}(\omega) d\xi d\omega$$
$$= \int_{\mathbb{R}^d} |\hat{g}(\omega)|^2 \int_{\mathbb{R}^d} \hat{u}(\omega+\xi) \overline{\hat{v}}(\omega+\xi) d\xi d\omega$$
$$=\Vert \hat{g} \Vert_2^2(\hat{u},\hat{v})_2$$
$$=\Vert \hat{g} \Vert_2^2(u,v)_2$$

\hfill$\blacksquare$


Vemos entonces que la transformada de Fourier de tiempo reducido es una isometría, por lo que podemos calcular la fórmula de inversión:

\textbf{Corolario 1.} Para $u,g \in L^2(\mathbb{R}^d)$ con $\Vert g \Vert_2 = 1$, tenemos que la fórmula de inversión es

$$u(x) = \frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} G_g u(\xi,t) g(x-t) e^{ix\xi} d\xi dt \text{    para casi todo x.}$$


\textbf{Demostración.} Dado que $g$ está normalizada, $G_g$ es una isometría, por lo que sólo nos queda calcular el operador adjunto. Para $u \in L^2(\mathbb{R}^d)$ y $F \in L^2(\mathbb{R}^{2d})$, tenemos que

$$ (u, G_g^*F)_{L^2(\mathbb{R}^d)} = (G_g u,F)_{L^2(\mathbb{R}^{2d})}$$
$$ = \int_{\mathbb{R}^{2d}} G_g u(\xi,t) \overline{F(\xi,t)} d\xi dt$$
$$ = \int_{\mathbb{R}^{2d}} \frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^{2d}} u(x) \overline{g(x-t)} e^{-ix \xi}dx \overline{F(\xi,t)} d\xi dt$$
$$ = \int_{\mathbb{R}^d} u(x) \overline{\frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^{2d}} F(\xi,t) e^{ix\xi}g(x-t)d\xi dt} dx,$$

lo que implica

$$G_g^*F(x) = \frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^{2d}} F(\xi,t) e^{ix\xi}g(x-t)d\xi dt$$

\hfill$\blacksquare$

La transformada ventana de Fourier no se aplica en procesamiento de imágenes generalmente. Hay varios motivos: en primer lugar, la transformación de una imagen produce una función de cuatro variables. Esto nos lleva a un gran consumo de memoria y perdemos gran parte de la capacidad de visualización. Por otra parte, la discretización de esta transformada no es para nada trivial y no hay un análogo directo para series de Fourier o transformada discreta. Para más información, véase (\cite{ftta}).

\item Transformada Wavelet continua

Nos centramos a partir de ahora en la herramienta principal en cuanto a la extracción de características se refiere. Mientras que la transformada de tiempo reducido de Fourier usa una ventana fija para localizar la función de interés, la transformada wavelet utiliza funciones variando sus anchuras (Figura 3.3). 

 \begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.42]{widths.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Transformada ventana (izquierda) y transformada wavelet (derecha)} 
	\label{fig:widths}
\end{figure}

En el caso de que nos encontremos en dimensión mayor que uno (nuestro caso), la transformada se puede definir de varias formas. Nos ocupamos primero de la transformada unidimensional:

\textbf{Definición 5.} Sean $u, \psi \in L^2(\mathbb{R},\mathbb{R})$. Para $b \in \mathbb{R}, a>0$ la transformada wavelet de $u$ con $\psi$ se define como

$$L_\psi u(a,b) = \int_{\mathbb{R}} u(x) \frac{1}{\sqrt{a}} \psi\left(\frac{x-b}{a}\right) dx$$

La transformada wavelet depende del parámetro espacial $b$ y el parámetro de escala $a$. Como en casos anteriores, podemos representarla de múltiples formas:

$$L_\psi (a,b) = \frac{1}{\sqrt{a}} (u, T_{-b}D_{1/a}\psi)_{L^2(\mathbb{R})}$$
$$=\frac{1}{\sqrt{a}} (u*D_{-1/a}\psi)(b))$$

Como la transformada de Fourier de tiempo reducido, la transformada wavelet posee cierta isometría, sin embargo, lo hace respecto de una medida ponderada. Introduzco el siguiente espacio:

$$L^2 \left( [0,\infty[ \times \mathbb{R}, \frac{da db}{a^2}\right) = \{ F: [0,\infty[ \times \mathbb{R} \mapsto \mathbb{R}: \int_{\mathbb{R}} \int_{\mathbb{R}^+} |F(a,b)|^2 \frac{da db}{a^2} < \infty \} $$

El producto escalar en este espacio es

$$(F,G)_{L^2 \left( [0,\infty[ \times \mathbb{R}, \frac{da db}{a^2}\right)} = \int_{\mathbb{R}} \int_{\mathbb{R}^+} F(a,b) G(a,b) \frac{da db}{a^2}$$

\textbf{Teorema 4.} Sea $u, \psi \in L^2(\mathbb{R})$ con 

$$ 0 < c_\psi = 2\pi \int_{\mathbb{R}^+} \frac{|\hat{\xi}(\xi)|^2}{\xi} d\xi < \infty$$

Entonces 

$$L_\psi : L^2(\mathbb{R}) \mapsto L^2 \left( [0,\infty[ \times \mathbb{R}, \frac{da db}{a^2}\right)$$

es una función lineal y se tiene que

$$(L_\psi u, L_\psi v)_{L^2 \left( [0,\infty[ \times \mathbb{R}, \frac{da db}{a^2}\right)} = c_\psi(u,v)_{L^2\mathbb{R}}$$

\textbf{Demostración.} Usamos el producto escalar de la transformada wavelet, las propiedades de la transformada de Fourier y la fórmula de Plancherel para obtener

$$L_\psi u (a,b) = \frac{1}{\sqrt{a}}(u,T_{-b}D_{1/a}\psi)_{L^2(\mathbb{R})}$$
$$ = \frac{1}{\sqrt{a}} (\hat{u}, F(T_{-b}D_{1/a}\psi))_{L^2(\mathbb{R})}$$
$$ = \frac{1}{\sqrt{a}} (\hat{u},aM_{-b}D_a\hat{\psi})_{L^2(\mathbb{R})}$$
$$=\sqrt{a} \int_{\mathbb{R}} \hat{u}(\psi)e^{ib\psi}\overline{\hat{\psi}(a\xi)} d\xi$$
$$= \sqrt{a 2\pi} F^{-1} (\hat{u}\overline{D_a \hat{\psi}})(b)$$

Ahora, calculamos lo siguiente:

$$(L_\psi u, L_\psi v)_{ L^2 \left( [0,\infty[ \times \mathbb{R}, \frac{da db}{a^2}\right)} = \int_{\mathbb{R}}
\int_{\mathbb{R}^+} L_\psi u(a,b) L_\psi v(a,b) \frac{da db}{a^2}$$
$$= 2\pi \int_{\mathbb{R}^+} \int_{\mathbb{R}} a F^{-1}(\hat{u}\overline{D_a \hat{\psi}})(b)\overline{F^{-1}(\hat{v}\overline{D_a \hat{\psi}})(b)} db \frac{da}{a^2}$$
$$ = 2\pi  \int_{\mathbb{R}^+} \int_{\mathbb{R}}
a \hat{u}(\xi) \overline{\hat{\psi}(a\xi)}\overline{\hat{v}(\xi)\overline{\hat{\psi}(a\xi)}} d\xi \frac{da}{a^2}$$
$$ = 2\pi \int_{\mathbb{R}} \hat{u}(\xi) \overline{\hat{v}(\xi)} \int_{\mathbb{R}^+} \frac{|\hat{\psi}(a\xi)|^2}{a} da d\xi$$

Un cambio de variable y $|\hat{\psi}(-\xi)| = |\hat{\psi}(\xi)|$ lleva 

$$\int_{\mathbb{R}^+} \frac{|\hat{\psi}(a\xi)|^2}{a} da = \int_{\mathbb{R}^+} \frac{|\hat{\psi}(a|\xi|)|^2}{a} da = \int_{\mathbb{R}^+} \frac{|\hat{\psi}(\omega)|^2}{\omega} d\omega = \frac{c_\psi}{2\pi}$$

Aplicando la fórmula de Plancherel obtenemos lo que buscábamos.

\hfill$\blacksquare$

Que $c_\psi < \infty$ asegura que $L_\psi$ es una función continua y $c_\psi > 0$ garantiza la existencia de la inversa de $L_\psi$. \\

\textbf{Definición 6} La condición

$$0 < c_\psi = 2\pi \int_{\mathbb{R}^+} \frac{|\hat{\psi}(\xi)|^2}{\xi} d\xi < \infty$$

se llama la condición de admisibilidad y la función $\psi$ que la satisface se le llama \textit{wavelets}.

La condición de admisibilidad dice que, alrededor de cero, la transformada de Fourier de una wavelet tiende suficientemente rápido a cero, $\hat{\psi}(0) = 0$.

\textbf{Corolario 2} Sea $u, \psi \in L^2(\mathbb{R})$ y $c_\psi = 1$. Entonces

$$u(x) = \int_{\mathbb{R}} \int_{\mathbb{R}^+} L_\psi u(a,b) \frac{1}{\sqrt{a}} \psi\left(\frac{x-b}{a}\right) \frac{da db}{a^2}$$

\textbf{Desmotración.} Calculamos el adjunto de la transformada Wavelet. Para $u \in L^2(\mathbb{R})$, y $F \in L^2 \left( [0,\infty[ \times \mathbb{R}, \frac{da db}{a^2}\right)$,

$$(L_\psi u, F)_{L^2 \left( [0,\infty[ \times \mathbb{R}, \frac{da db}{a^2}\right)} = \int_{\mathbb{R}} \int_{\mathbb{R}^+} \int_{\mathbb{R}} u(x) \frac{1}{\sqrt{a}} \psi\left(\frac{x-b}{a}\right)dx F(a,b) \frac{da db}{a^2}$$
$$ = \int_{\mathbb{R}} u(x) \int_{\mathbb{R}} \int_{\mathbb{R}^+} F(a,b) \frac{1}{\sqrt{a}} \psi\left(\frac{x-b}{a}\right) \frac{da db}{a^2}dx $$

Esto implica que

$$L_\psi^* F(x) = \int_{\mathbb{R}} \int_{\mathbb{R}^+} F(a,b) \frac{1}{\sqrt{a}} \psi\left(\frac{x-b}{a}\right) \frac{da db}{a^2}$$

como queríamos demostrar.

\hfill$\blacksquare$

Presento ahora la wavelet más sencilla:

\textbf{Haar wavelet:} 

$$\psi(x) = \begin{cases}
	1 \  \ \ si \ 0 \leq x < 0.5 \\
	-1 \ \  si \ 0.5 \leq x < 1 \\
	0 \ \ en \ otro \ caso
\end{cases}$$

Es discontinua pero de soporte compacto:

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.42]{haar.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Wavelet Haar} 
	\label{fig:haar}
\end{figure}

\item Transformada discreta Wavelet

La transformada continua Wavelet es una representación redundante. La pregunta que surge es si sería suficiente con conocer la transformada wavelet de una función en un subconjunto de $[0,\infty[ \times \mathbb{R}$. Este es el caso para algunos subconjuntos discretos. Veamos que bajo ciertas condiciones, las funciones

$$\{\psi_{j,k}(x) = 2^{-j/2}\psi(2^{-j}x-k) | j,k \in \mathbb{Z}\}$$

forman una base ortonormal de $L^2(\mathbb{R})$. Para demostrarlo, primero necesitamos introducir ciertos conceptos. El primo de ellos es la noción de serie wavelet y transformada discreta wavelet, a saber, lo conocido como ``análisis multiescala''. \\

\textbf{Definición 7.} Una sucesión $(V_j)_{j \in \mathbb{Z}}$ de subconjuntos cerrados de $L^2(\mathbb{R})$ es llamado un análisis multiescala si satisface las siguientes condiciones:

\begin{itemize}
	\item Invarianza por traslaciones: $$\forall j,k \in \mathbb{Z}, u_j \in V_j \Leftrightarrow T_{2^jk}u \in V_j$$
	\item Inclusión: $$\forall j \in \mathbb{Z}, V_{j+1} \subset V_{j}$$
	\item Escala: $$\forall j \in \mathbb{Z}, u \in V_{j} \Leftrightarrow D_{1/2}u \in V_{j+1}$$
	\item Intersección trivial: $$\cap_{j \in \mathbb{Z}} V_j = \{0\}$$
	\item Completitud: $$\overline{\cup_{j \in \mathbb{Z}} V_j} = L^2(\mathbb{R})$$
	\item Base ortonormal: Existe una función $\phi \in V_0$ tal que las funciones $\{T_k \phi | k \in \mathbb{Z} \}$ forman una base ortonormal de $V_0$
	La función $\phi$ es llamada un generador o una función de escala del análisis multiescala.
\end{itemize}

Hagamos algunas puntualizaciones sobre esta definición. Los espacios $V_j$ son invariantes por traslación respecto de las traslaciones diádicas $2^j$. Además, están encajadas entre sí y se hacen más pequeñas conforme crece $j$. Si notamos $P_{V_j}$ como la proyección ortogonal sobre $V_j$, entonces para todo $u$ 

$$\lim_{j \mapsto \infty} P_{V_j} u = 0, \lim_{j \mapsto -\infty} P_{V_j} u = 0$$

\textbf{Definición 8.} Sea $(V_j)_{j \in \mathbb{Z}}$ un análisis multiescala. Sean $W_j$ los espacios definidos como los complementos ortogonales de $V_j, V_{j-1}$

$$V_{j-1} = V_j \oplus W_j, V_j \perp W_j$$

El espacio $V_j$ es llamado el espacio aproximación a la escala $j$; el espacio $W_j$ es llamado el espacio wavalet a la escala $j$.

La definición de $W_j$ directamente implica

$$V_j = \oplus_{m \in \mathbb{Z}} W_m$$

y debido a la completitud de $V_j$,

$$L^2(\mathbb{R}) =   \oplus_{m \in \mathbb{Z}} W_m$$

Además, tenemos que $P_{V_{j-1}} = P_{V_j} + P_{W_j}$ y por tanto

$$ P_{W_j} = P_{V_{j-1}} - P_{V_j}$$

Podemos ahora representar cada $u \in L^2(\mathbb{R})$ a través de los espacios $V_j, W_j$ 

$$ u = \sum_{j \in \mathbb{Z}}P_{W_j}u = P_{V_m}u + \sum_{j \leq m}P_{W_j}u$$

Estas ecuaciones justifican el nombre de análisis multiescala: los espacios $V_j$ permiten una aproximación sistemática de funciones a diferentes escalas.

\textbf{Teorema 5.} Sea $(V_j)$ un análisis multiescala con generador $\phi$ tal que $\phi$ satisface 

$$\phi(x) = \sum_{j \in \mathbb{Z}} h_k \sqrt{2} \phi(2x-k)$$ 
con una sucesión $(h_k)$. Más aún, sea $\psi \in V_{-1}$ definido como

$$\psi(x) = \sqrt{2} \sum_{k \in \mathbb{Z}} (-1)^k h_{1-k}\phi(2x-k)$$

Entonces:

\begin{enumerate}
	\item El conjunto $\{\psi_{j,k}: k\in \mathbb{Z} \}$ es una base ortonormal de $W_j$.
	\item El conjunto $\{\psi_{j,k}: k\in \mathbb{Z} \}$ es una base ortonormal de $L^2(\mathbb{R})$.
	\item La función $\psi$ es una wavelet con $c_\psi = 2 \log 2$.
\end{enumerate}

\textbf{Demostración.} Primero vemos que para todo $k \in \mathbb{Z}$, tenemos que

$$(\psi, \phi_{k,0}) = 0,$$
$$(\psi, \psi_{k,0}) = \delta_{0,k}$$

La primera ecuación implica que $\psi \perp V_0$, y por tanto tenemos que $\psi \in W_0$. La segunda ecuación implica la ortonormalidad de las traslaciones de $\psi$. \\

Ahora demostramos que el sistema $\{\psi_{k,0} | k \in \mathbb{Z} \}$ es completo en $W_0$. Dado que $V_{-1} = V_0 \oplus W_0$, es equivalente demostrar que el sistema $\{\phi_{k,0} | k \in \mathbb{Z} \}$ es completo en $V_{-1}$. Esto último lo obtenemos viendo que $\phi_{-1,0}$ se puede representar como $\{\phi_{k,0}, \psi_{k,0} | k \in \mathbb{Z} \}$. Para ello, calculamos lo siguiente:

$$\sum_{k \in \mathbb{Z}} | (\phi_{-1,0}, \psi_{k,0})|^2 +  | (\phi_{-1,0}, \phi_{k,0})|^2$$
$$= \sum_{k \in \mathbb{Z}}| \sum_{l \in \mathbb{Z}} h_l (\phi_{-1,0},\phi_{-1,l+2k}) |^2 + | \sum_{l \in \mathbb{Z}}(-1)^l h_{1-l} (\phi_{-1,0},\phi_{-1,l+2k}) |^2$$
$$= \sum_{k \in \mathbb{Z}} h_{-2k}^2 + h_{1+2k}^2 = \sum_{k \in \mathbb{Z}}h_k^2$$

Se puede ver que $\sum_{k \in \mathbb{Z}} h_k^2 = 1$, y debido que $\Vert \phi_{-1,0} \Vert = 1$, se sigue que el sistema $\{\phi_{k,0}, \psi_{k,0} | k \in \mathbb{Z} \}$ es completo en $V_{-1}$.

Para el tercer punto, hago referencia a \cite{meyer}


\hfill$\blacksquare$

Por último, desarrollo la transformada wavelet discreta bidimensional, la herramienta definitiva para nuestro estudio 2D de las MRI cerebrales:

\item La Transformada Wavelet Discreta Bidimensional

Basándonos en una base wavelet ortonormal $\{\psi_{j,k} | j,k \in \mathbb{Z} \}$ de $L^2)(\mathbb{R})$, podemos construir una base ortonormal de $L^2(\mathbb{R}^2)$ a través de todos los productos tensoriales. Las funciones

$$(x_1,x_2) \mapsto \psi_{j_1,k_1}(x_1), \psi_{j_2,k_2}(x_2), \ \ \  j_i,k_i \in \mathbb{Z}$$

forman una base ortonormal de $L^2(\mathbb{R}^2)$.

En el mismo sentido, podemos constituir un análisis multiescala de $L^2(\mathbb{R}^2)$: para un análisis multiescala $(V_j)$ de $L^2(\mathbb{R}^2)$, creamos los espacios

$$V_j^2 = V_j \otimes V_j \subset L^2(\mathbb{R}^2)$$

que están definidos dado que las funciones

$$\Phi_{j,k}: (x_1,x_2) \mapsto \phi_{j,k_1}(x_1) \phi_{j,k_2}(x_2), \vspace{0.5cm} k= (k_1,k_2) \in \mathbb{Z}^2$$

forman una base ortonormal de $V_j^2$. Esta construcción también es llamada un producto tensorial para espacios de Hilbert separables (\cite{weid}). \\

En el caso bidimensional, los espacios wavelet, es decir, los complementos ortogonales de $V_j^2$ en $V_{j-1}^2$ tienen una estructura más compleja. Definimos el espacio wavelet $W_j^2$ 

$$V_{j-1}^2 = V_j^2 \oplus W_j^2$$

donde el superíndice 2 significa en un caso un producto tensorial y en el otro representa un nombre. Por otra parte, $V_{j-1} = V_j \oplus W_j$, de lo que obtenemos

$$V_{j-1}^2 = (V_j \oplus W_j) \otimes (V_j \oplus W_j)$$
$$ = (V_j \otimes V_j) \oplus (V_j \otimes W_j) \oplus (W_j \otimes V_j) \oplus (W_j \otimes W_j)$$

y llamando

$$H_j^2 = V_j \otimes W_j, \hspace{0.2cm} S_j^2 = W_j \otimes V_j, \hspace{0.2cm} D_j^2 = W_j \otimes W_j$$

podemos expresar

$$V_{j-1}^2 = V_j^2 \oplus H_j^2 \oplus S_j^2 \oplus D_j^2$$

Denotando la functión de escala de $(V_j)$ como $\phi$ y la correspondiente wavelet como $\psi$, definimos tres funciones:

$$\psi^1(x_1,x_2) = \phi(x_1) \psi(x_2), \hspace{0.2cm} \psi^2(x_1,x_2) = \psi(x_1) \phi(x_2), \hspace{0.2cm} \psi^3(x_1,x_2) = \psi(x_1) \psi(x_2)$$

Para $m \in {1,2,3}, j \in \mathbb{Z}, k \in \mathbb{Z}^2$, establecemos,

$$\psi_{j,k}^2(x_1,x_2) = 2^{-j} \psi^m(2^{-j}x_1-k_1,2^{-j}x_2-k_2)$$

Se puede probar que las funciones $\{\psi_{j,k}^1 | k \in \mathbb{Z}^2 \}$ forma una base ortonormal de $H_j^2$, las funciones $\{\psi_{j,k}^2 | k \in \mathbb{Z}^2 \}$ constituyen una base de $S_j^2$ y las funciones $\{\psi_{j,k}^3 | k \in \mathbb{Z}^2 \}$, de $D_j^2$. Por tanto

$$\{\psi_{j,k}^m | m = 1,2,3, k\in \mathbb{Z}^2, j \in \mathbb{Z}\}$$

constituye de forma natural una base ortonormal de $L^2(\mathbb{R}^2)$.

Observamos que los espacios wavelet $W_j^2$ están generados por tres wavelets: $\psi^1, \psi^2$ y $\psi^3$ juntas con sus escalas y traslaciones. Los espacios $H_j^2$ contienen los detalles horizontales en la escala $j$ (en la dirección $x_1$) , los espacios $S_j^2$ los detalles verticales (dirección $x_2$) y los espacios $D_j^2$ los detalles en diagonal:

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.5]{wav.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{La transformada wavelet discreta 2D de una imagen a través de la wavelet Haar} 
	\label{fig:wav}
\end{figure}

Este tipo de análisis multiescala bidimensional es fácil de implementar algorítmicamente: basado en la aproximación de los coeficientes $c^j$, calculamos los coeficientes $c^{j+1}$ es una escala mayor además de los tres coeficientes de detalles $d^{1,j+1}, d^{2,j+1}, d^{3,j+1}$. En la práctica, esto se consigue con la concatenación de la descomposición wavelet unidimensional a lo largo de las columnas y las filas:

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.5]{esquema.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Esquema de actuación algorítmica para calcular transformada 2D} 
	\label{fig:esquema}
\end{figure}
\end{itemize}

\newpage

\subsubsection{Selección de características}

Una vez que hemos aplicado la transformada discreta wavelet 2D sobre cada una de los slices y hemos configurado nuestro dataset, nos encontramos con un conjunto de datos de enormes dimensiones, repleto de información (gran parte, probablemente, redundante o inútil en nuestro proyecto) que necesita ser estudiada. Esta basta cantidad de datos tiene dos grandes inconvenientes. El primero de ellos es que los algoritmos de aprendizaje no serán capaces de encontrar las características determinantes a la hora de clasificar instancias debido al gran ruido que tenemos (efecto Hughes). Por su parte, el segundo es un problema pragmático: los clasificadores no van a tener un rendimiento adecuado y tardarán demasiado tiempo en ejecutarse. Por tanto, es necesaria una selección de las características más importantes para así garantizar un mejor rendimiento y unas conclusiones más certeras. Existen multitud de técnicas en estadística multivariante para reducir la dimensionalidad. De entre ellas destaca Análisis de Componentes Principales (PCA) y por ello la utilizamos. PCA es una herramienta eficiente para reducir la dimensionalidad de un dataset formado un gran número de variables correladas entre sí. El objetivo es transformar el conjunto de datos en uno nuevo constituido por las variables ordenadas según el grado de importancia o varianza de las mismas. Esta técnica tiene tres efectos:

\begin{itemize}
	\item Ortogonaliza las componentes de los vectores de entrada para que no tengan correlación entre ellas.
	\item Ordena el resultado de las componentes ortogonales de mayor a menor en variación o importancia.
	\item Elimina aquellas componentes que aportan menor variación.
\end{itemize}

Téngase en cuenta que los vectores de entrada deben estar normalizados con media cero y desviación típica uno antes de utilizar el algoritmo. La normalización se realiza tal y como se explicó en la sección anterior. \\

PCA encuentra una función a través de la cual incluye grandes cantidades de información en muestras analizadas. Sin embargo, como las funciones son combinaciones lineales de las características originales, es difícil (por no decir imposible) interpretar las variables resultantes de la transformación. Desde el punto de vista clínico, este hecho hace que el estudio pierda valor médico, aunque es realmente necesario debido al volumen de datos que tenemos. De manera intuitiva, dado un espacio de características n-dimensional, PCA ajusta un elipsoide de la misma dimensión que contiene un porcentaje prefijado de la varianza de los datos y cada eje se convierte en una componente principal, por lo que si un eje es pequeño, su varianza también lo será. \\

Desarrollo ahora el contenido matemático de Análisis de Componentes Principales (\cite{amsa}, \cite{ms}).
\newpage
	
	Un vector aleatorio es un vector cuyas componentes son variables aleatorias. Por tanto, la media del vector es un vector formado por las medias de cada componente. Si $X = (X_1,\dots,X_n)$ es un vector aleatorio y $\mu = (\mu_1,\dots,\mu_n)$ es su vector media, entonces
	
	$$\mu_i = E(X_i) \hspace{0.5cm} i =1, \dots, n$$
	
	Sin embargo, la analogía para la varianza no es tan obvia. Definimos $var(X)$ como una matriz donde, para cada $i,j$, el elemento es $cov(X_i,X_j)$. Esto es lo que se llama la matriz de covarianzas de $X$. Esta matriz es siempre simétrica. \\
	
	Si $X$ e $Y$ son variables aleatorias e $Y$ es una función lineal de $X$, esto es, $Y = aX + b$ para alguna constante $a$ y $b$, entonces
	
	$$E(Y) = a E(X) + b$$
	$$var(Y) = a^2 var(X)$$
	$$\sigma(Y) =  |a| \sigma(X)$$
	
	Si $X,Y$ son vectores aleatorios, entonces decimos que $Y$ es una función lineal (afín) de $X$ cuando $Y = AX +b$, para alguna matriz constante $A$ y algún vector constante $b$. Tenemos que
	
	$$E(Y) = A E(X) + b $$
	$$var(Y) = A var(X) A^T$$
	
	
	Consideremos el caso en que $Y = AX$, siendo $A$ una matriz fila. Por tanto, $AX$ es un escalar. Por convención, $A = a^T$, siendo $a$ un vector columna. De la igualdad anterior tenemos que
	
	$$ 0 \leq var(Y) = a^T var(X) a$$
	
	teniendo en cuenta que la varianza de una variable aleatoria siempre es no negativa. Esta propiedad tiene un nombre. Un matriz simétrica $V$ se dice que es semi-definida positiva si 
	$$a^T V a \geq 0 \hspace{1cm} \forall a$$
	
	Por tanto, la matriz de covarianzas es simétrica y semi-definida positiva. \\
	
	Tratamos ahora la descomposición espectral. Toda matriz simétrica tiene una descomposición espectral
	
	$$A = ODO^T$$
	donde $D$ es diagonal y $O$ una matriz ortogonal. Pensando en $O$ como un cambio de sistema de coordenadas, encontramos una correspondencia con los movimientos rígidos (rotaciones) con ejes coordenados perpendiculares. La descomposición espectral puede utilizarse para determinar si una matriz es semi-definida positiva. 
	
	\textbf{Proposición.} Una matriz diagonal es semi-definida positiva si y solamente si todos sus elementos son no negativos.
	
	\textbf{Demostración} Basta ver que 
	$$a^T D a = \sum_{i} a_i^2 d_ii.$$
	\hfill$\blacksquare$
	
	En general, una matriz simétrica es semi-definida positiva si y solamente si la matriz diagonal de su descomposición espectral es semi-definida positiva, ya que
	
	$$a^T A a = a^T ODO a = b^t D b$$
	
	donde $b= O^Ta$ y $a = Ob$. \\
	
	Tratamos ahora los valores y vectores propios. Multiplicando a la derecha por $O$, tenemos que
	
	$$AO = ODO^TO = OD$$
	
	Si observamos este resultado por columnas $w_i \in O$, tenemos que
	$$ A w_i = \lambda_i w_i$$
	
	donde $\lambda_i$ es el i-ésimo elemento de $D$.
	
	Por tanto, los elementos de $D$ son los valores propios de $A$ y las columnas de $O$, sus correspondientes vectores propios. \\
	
	Una vez introducidos estos sencillos conceptos, nos adentramos en Componentes Principales. Si $X$ es un vector aleatorio de varianza finita, sea 
	$$var(X) = ODO^T$$
	la descomposición espectral de su matriz de covarianzas.
	
	Considero el vector aleatorio $Y = O^T X$. Entonces
	
	$$var(Y) = O^T var(X) O = O^T ODO^T O = D$$
	
	ya que $O$ es ortogonal. Por tanto, $Y$ tiene una matriz de covarianzas diagonal. Por tanto, como los elementos que están fuera de la diagonal de la matriz de covarianzas son las propias covarianzas de las variables, las componentes de $Y$ no están correladas. Y ya que los elementos de la diagonal de la matriz de covarianzas son la varianza de cada variable y los elementos de $D$ son los valores propios de $var(X)$, las varianzas de los componentes de $Y$ son los valores propios de la matriz de covarianzas de $X$. \\
	
	Los elementos de $Y$ son las llamadas componentes principales de $X$. Dado que una matriz ortogonal es invertible, tenemos que $X = OY$. Esto expresa una variable aleatoria cualquiera como una combinación lineal de variables aleatorias independientes. Este proceso de  hacer la descomposición espectral de la matriz de covarianzas de X es el llamado Análisis de Componentes Principales.
	
	PCA se usa como método de reduccion de dimensionalidad. Si tomamos unos pocos componentes principales, obtenemos una explicación simple de la estructura de $X$ a través de unas pocas variables.Deben ordenarse las componentes de $Y$ poniendo en primer lugar las componentes más grandes (los mayores valores propios). Por último, tratamos la varianza explicada. Sea $\Vert · \Vert$ la norma Euclidea de un vector, por lo que
	
	$$ \Vert Y \Vert^2 = Y^T Y = \sum_{i} Y_i^2$$
	
	Como las componentes de $Y$ son independientes, tenemos que 
	
	$$E(\Vert Y - v \Vert^2) = \sum_{i=1}^{n} \lambda_i$$
	
	donde $v = E(Y)$. Dado que una transformación ortogonal es una rotación (isometría), no afecta a distancias ni longitudes. Si $\mu = E(X)$, tenemos
	
	$$E(\Vert X -\mu \Vert^2) = E((X-\mu)^T (X-\mu)$$
	$$ = E((Y-v)^T O^T (Y-v)$$
	$$ = E((Y-v)^T (Y-v)) $$
	$$E(\Vert Y-v \Vert^2)$$
	
	Dado que $Y = O^TX$, $X = OY$ y $\mu = O v$. De forma similar, si $\overline{\mu} = E(\overline{X})$, entonces
	
	$$E(\Vert \overline{X}- \overline{\mu} \Vert^2) = \sum_{i=1}^{k} \lambda_i$$
	
	La fracción de varianza de $X$ explicada por las primeras $k$ componentes principales es
	$$\frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{n} \lambda_i}$$
	

En nuestro proyecto, utilizamos siempre PCA con una varianza explicada del 95\%.
\newpage
\subsection{Clasificación}

En esta sección expongo los distintos algoritmos que hemos usado para clasificar a los sujetos según las imágenes MRI. El primero de ellos será SVM. Más tarde, nos enfrentaremos a la necesidad de ajustar los hiperparámetros del modelo optimizando así los resultados. De igual manera, nos enfrentaremos al problema del sobreajuste, que solucionamos con Validación Cruzada de 10 particiones. Será necesaria también la mención de las redes neuronales profundas y, finalmente, la regresión logística.

\subsubsection{SVM}

SVM es un clasificador (aprendizaje supervisado) muy potente introducido en 1995. Consigue resultados buenos en clasificación especialmente en diagnóstico médico. SVM se basa en el principio de minimización del riesgo estructural proveniente de la teoría del aprendizaje en estadística. Su objetivo es encontrar un hiperplano separador óptimo entre los miembros y los no miembros de una clase en un espacio de características de varias dimensiones. La entrada al algoritmo está formada por un subconjunto de características elegidas por medio de la extracción y selección de las mismas. En nuestro caso, las clases son Control (sano) y PD (enfermo de PD), por lo que dividir los pacientes sigue siendo nuestro objetivo. Veamos su fundamentación matemática (\cite{esl}): \\

Supongamos que disponemos de un conjunto de instancias divididas en dos clases linealmente separables. Nuestro conjunto de entrenamiento está formado por $N$ parejas $(x_1,y_1),(x_2,y_2),\dots (x_n,y_n)$ con $x_i \in \mathbb{R}^p, y_i \in \{1,-1\}$. Definimos el hiperplano

$$\{x:f(x) = x^T \beta + \beta_0 = 0 \}$$

donde $\beta$ es un vector unitario. La función que clasifica inducida por $f(x)$ es 

$$G(x) = sign[x^T \beta + \beta_0]$$


Esta última expresión representa la distancia con signo de un punto $x$ al hiperplano $f(x) = x^T \beta + \beta_0=0$. Dado que las clases son separables, podemos encontrar una solución $f(x) = x^T \beta + \beta_0$ con $y_if(x_i) > 0 \forall i$. En consecuencia, somos capaces de encontrar un hiperplano que genera el mayor \textit{margen} entre las instancias de entrenamiento para las clases 1 y -1 (Figura 3.7)

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.5]{hip.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Hiperplano separador con el margen y los vectores soporte (aquellos que definen la región)} 
	\label{fig:hip}
\end{figure}

El problema de optimización es el siguiente: 
$$max_{\beta, \beta_0, \Vert \beta \Vert = 1} M$$
$$\text{sujeto a } y_i(x_i^T\beta + \beta_0) \geq M, i=1,\dots,N$$

La banda tiene una anchura de $2M$ ($M = \frac{1}{\beta}$ desde cada clase) y se llama margen. Por la definición de $M$, es posible reformular el problema de una forma más sencilla

$$min{\beta, \beta_0} \beta$$
$$\text{sujeto a } y_i(x_i^T\beta + \beta_0) \geq M, i=1,\dots,N$$

Este es un problema de optimización con funciones convexas que se resuelve con los procedimientos clásicos del análisis real de varias variables. \\

Supongamos ahora que las clases se solapan en el espacio de características. Una forma de enfrentarnos a ese solapamiento es maximizar $M$ permitiendo a algunos puntos estar mal clasificados (quedarse en el lado erróneo del margen). Si definimos esa anomalía a a través de una variable $\xi = (\xi_1,\dots,\xi_n)$, podemos representar el problema de restricciones anterior de la siguiente manera

$$y_i(x_i^T\beta+\beta_0) \geq M(1-\xi_i)$$
$\forall i, \xi_i \geq 0, \sum_{i=1}^{N} \xi_i \leq cte$.

La idea es la siguiente. El valor $\xi_i$ en la restricción $y_i(x_i^T\beta+\beta_0) \geq M(1-\xi_i)$ es la cantidad proporcional por la que la predicción $f(x_i) = x_i^T \beta + \beta_0$ está en el lado erróneo del margen. Por tanto, acotando $\sum \xi_i$, acotamos la cantidad proporcional total a las predicciones mal situadas. Estos errores en la clasificación ocurren cuando $\xi_i > 1$, así que acotar $\sum \xi_i$ por una constante $K$ acota superiormente el número de instancias mal clasificadas por $K$.

Como antes, podemos eliminar la restricción sobre la norma de $\beta$, ya que $M = \frac{1}{\Vert \beta \Vert}$ y escribimos

$$min \Vert \beta \Vert$$
$$\text{sujeto a } y_i(x_i^T \beta + \beta_0) \geq 1-\xi_i \forall i \text{  y  } \xi_i \geq 0, \sum \xi_i \leq cte.$$ 

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.5]{hip2.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Hiperplano separador con el margen y clases solapadas} 
	\label{fig:hip2}
\end{figure}

El problema anterior es cuadrático con inecuaciones lineales como restricciones, luego es un problema de optimización convexo. Para resolverlo, utilizamos los multiplicadores de Lagrange. Reescribimos la expresión de una forma más conveniente desde el punto de vista computacional

$$min_{\beta,\beta_0} \frac{1}{2} \Vert \beta \Vert^2 + C \sum_{i=1}^{N} \xi_i$$
$$\text{sujeto a } \xi_i \geq 0, y_i(x_i^T \beta + \beta_0) \geq 1- \xi_i \forall i$$

donde $C$ es un parámetro que refleja el coste (para un problema donde es linealmente separable, $C= \infty$).

El lagrangiano es

$$L_p = \frac{1}{2} \Vert \beta \Vert^2 + C \sum_{i=1}^{N} \xi_i - \sum_{i=1}^{N}\alpha_i[y_i(x_i^T \beta + \beta_0)-(1-\xi_i)] - \sum_{i=1}^{N} \mu_i \xi_i$$

en la que minimizamos respecto de $\beta, \beta_0$ y $\xi_i$. Derivando e igualando a cero obtenemos

$$\beta = \sum_{i=1}^{N} \alpha_i y_i x_i,$$
$$0 = \sum_{i=1}^{N} \alpha_i y_i$$
$$\alpha_i = C-\mu_i \forall i$$

además de las restricciones $\alpha_i,\mu_i,\xi_i \geq 0 \forall i$. Sustituyendo en la expresión anterior, obtenemos que

$$L_D = \sum_{i=1}^{N}\alpha_i - \frac{1}{2}\sum_{i=1}^{N}\sum_{i'=1}^{N}\alpha_i \alpha_{i'} y_i y_{i'}x_i^T x_{i'}^T$$

Ahora, maximizamos $L_D$ sujeto a que $0 \leq \alpha_i \leq C$ y $\sum_{i=1}^{N} \alpha_i y_i=0$. Además de las ya presentadas, las condiciones de Karush-Kuhn-Tucker (\cite{kkt}) incluyen las restricciones

$$\alpha_i[y_i(x_i^T \beta + \beta_0)-(1-\xi_i)] = 0$$
$$\mu_i \xi_i = 0$$
$$y_i(x_i^T \beta + \beta_0)-(1-\xi_i) \geq 0$$

para $i = 1,\dots, N$\\

De la igualdad correspondiente a $\beta$, vemos que la única solución tiene la forma

$$\hat{\beta} = \sum_{i=1}^{N} \hat{\alpha_i} y_i x_i$$

para coeficientes $\hat{\alpha_i}$ distintos de cero que cumplen las restricciones citadas. Estas observaciones son los llamados vectores soporte. \\ 

\textbf{SVM y núcleos}\\

El clasificador descrito hasta ahora encuentra los bordes de los datos en el espacio de características de forma lineal. Podemos hacer este procedimiento más flexible aumentando el espacio de características usando extensiones basadas en polinomios o splines. Generalmente, los bordes lineales consiguen una mejor separación entre las clases del conjunto de entrenamiento en espacio mayores y se traducen en bordes no lineales en el espacio original. Una vez que las funciones base $h_m(x), m=1,\dots,M$ han sido seleccionadas, el procedimiento es igual que antes. Ajustamos el clasificador usando las características $h(x_i) = (h_1(x_i),\dots,h_M(x_i)), i = 1, \dots, N$ y producimos la función (no lineal) $\hat{f}(x) = h(x)^T \hat{\beta} + \hat{\beta_0}$. El target finalmente es, como antes,  $\hat{G}(x)= sign(\hat{f}(x))$. Utilicemos esta idea para extender el concepto de SVM. Si escribimos la función lagrangiana dual 

$$L_D = \sum_{i=1}^{N}\alpha_i - \frac{1}{2}\sum_{i=1}^{N}\sum_{i'=1}^{N}\alpha_i \alpha_{i'} y_i y_{i'}\langle h(x_i),h(x_{i'})\rangle$$

y utilizando la expresión de $f$, tenemos que

$$f(x) = h(x)^T \beta + \beta_0$$
$$= \sum_{i=1}^{N} \alpha_i y_i \langle h(x),h(x_i) \rangle + \beta_0$$

Como antes, dado $\alpha_i, \beta_0$ puede calcularse por medio de la ecuación $$ y_i f(x_i) = 1$$ $\forall x_i \text{ con } 0 < \alpha_i < C$.

Las dos expresiones anteriores involucran a $h$ a través de productos escalares. En realidad, no necesitamos conocer $h$ si no el núcleo

$$K(x,x) = \langle h(x),h(x') \rangle$$

que calcula los productos escalares en los espacios transformados. $K$ debe ser simétrica y semidefinida positiva. Existen tres núcleos popularmente utilizados en la literatura:
\begin{itemize}
		\item Polinomial de grado $d$: $K(x,x') = (1+\langle x,x'\rangle) ^d$
		\item Base radial: $K(x,x') = exp(-\gamma \Vert x-x'\Vert^2)$
		\item Red neuronal: $K(x,x') = tanh(k_1 \langle x, x' \rangle + k_2)$
\end{itemize}

Finalmente, la solución tiene la expresión

$$\hat{f}(x) = \sum_{i=1}^{N} \hat{\alpha_i}y_i K(x,x_i)+\hat{\beta_0}$$

\subsubsection{Validación cruzada con K-particiones}

Validación cruzada es una técnica de validación de modelos que determina la robustez de los resultados de nuestro análisis estadístico. De forma específica, hace más robusta la fase de entrenamiento haciendo particiones k sucesivas, a su vez,de entrenamiento y validación, de forma que k-1 partes quedan para entrenar y una para testear el modelo. El proceso es llevado a cabo k veces reduciendo así la variabilidad. En nuestro proyecto utilizamos 10 particiones.

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.5]{cv.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Ejemplo de validación cruzada} 
	\label{fig:cv}
\end{figure}

\subsubsection{Ajuste de hiperparámetros y GridSearch}

Mientras ejecutamos la validación cruzada de 10 particiones intentamos encontrar los mejores parámetros $C$ y $\gamma$ a través de un grid search. Grid Search es una técnica utilizada para la optimización de hiperparámetros, asegurando así que los modelos no sobreajustan. Este proceso consiste en una búsqueda exhaustiva en un rango definido para la pareja $(C,\gamma)$. Aquella pareja que consiga el mayor \textit{recall} (\textit{accuracy} no es la mejor medida en biomedicina, puesto que lo que queremos evitar son los falsos negativos) es seleccionada para el training. Los rangos de valores son los siguientes:
\begin{itemize}
	\item $C = \{2^{i},i=-2,\dots,11\} $
	\item $\gamma = \{2^{i}, i =-9,\dots,3\}$
\end{itemize}

\subsubsection{Regresión logística}

La última pieza de nuestro clasificador, antes de explicar la arquitectura completa, es la regresión logística (\cite{esl}). Los modelos de regresión logística surgen por el deseo de modelar la probabilidad a posteriori de las $K$ clases a través de una función lineal en $x$, mientras que al mismo tiempo se aseguran de que suman 1 y se mantienen en el intervalo [0,1] (es una probabilidad). el modelo tiene la forma

$$\log \frac{P(G=1| X =x)}{P(G=K|X=x} =  \beta_{10} + \beta_1^Tx$$
 $$\log \frac{P(G=2| X =x)}{P(G=K|X=x} =  \beta_{20} + \beta_2^Tx$$
 $$\vdots$$
 $$\log \frac{P(G=K-1| X =x)}{P(G=K|X=x} =  \beta_{(K-1)0} + \beta_{K-1}^Tx$$
 
 A pesar de que el modelo utiliza la última clase en el denominador de cada expresión, la elección de dicho denominador es arbitraria y la estimaciones son invariantes bajo esta elección. Un simple cálculo muestra que
 
 $$P(G=k| X=x) = \frac{exp(\beta_{k0}+\beta_k^T x)}{1+\sum_{l=1}^{K-1}exp(\beta_{l0}+\beta_l^Tx)}$$
  $$P(G=K| X=x) = \frac{1}{1+\sum_{l=1}^{K-1}exp(\beta_{l0}+\beta_l^Tx)}$$
  
  con $k=1,\dots,K-1$ y claramente suman 1. Para enfatizar la dependencia sobre el conjunto de parámetros $\theta = {\beta_{10},\beta_1^T,\dots,\beta_{(K-1)0},\beta_{K-1}^T}$, denotamos las probabilidades $P(G=k|X=x) = p_k(x;\theta)$. \\
  
  Cuando $K=2$ (nuestro caso), el modelo especialmente simple, ya que se trata exclusivamente de una función lineal. \\
  
  \textbf{Ajustando un modelo de regresión logística} \\
  
  Los modelos de regresión logística se ajustan habitualmente con el principio de máxima verosimilitud, usando la verosimilitud condicionada a G dado X. Dado que P(G|X) especifica completamente la distribución condicional, la utilización de la multinomial es adecuada. La \textit{log-verosimilitud} para $N$ observaciones es
  
  $$l(\theta) = \sum_{i=1}^{N} \log p_{g_i}(x_i;\theta)$$
  donde $p_k(x_i;\theta) = P(G=k|X=x_i)$. Nos centramos en $K=2$, puesto es el caso que nos ocupa. Supongamos que las dos clases que tenemos se codifican como 0/1, de forma que $y_i=1$ cuando $g_i=1$ y $y_i=0$ cuando $g_i=2$. Sea $p_1(x;\theta) = p(x;\theta), p_2(x;\theta) = 1 -p(x;\theta)$. La \textit{log-verosimilitud} se puede escribir como
  
  $$l(\beta) = \sum_{i=1}^{N} \{ y_i \log p(x_i;\beta)+(1-y_i) \log(1-p(x_i;\beta)) \}$$
  $$ = \sum_{i=1}^{N} \{y_i \beta^T x_i - \log(1+e^{\beta^T x_i}) \}$$
  
  Aquí, $\beta = \{\beta_{10},\beta_1 \}$ y asumimos que el vector de entrada $x_i$ incluye el término constante 1. Para maximizar la \textit{log-verosimilitud}, derivamos e igualamos a cero:
  
  $$\frac{\partial l(\beta)}{\partial \beta} = \sum_{i=1}^{N} x_i (y_i -p(x_i;\beta))=0$$
  
  siendo $p+1$ ecuaciones no lineales en $\beta$.  Para resolverlas utilizamos el algortimo de Newton-Raphson, que requiere el cálculo de la matriz Hessiana
  
  $$\frac{\partial^2 l(\beta)}{\partial \beta \partial \beta^T} = - \sum_{i=1}^{N} x_i x_i^T p(x_i;\beta)(1-p(x_i;\beta))$$
  
  Empezando con un $\beta^{old}$, tenemos que
  
  $$\beta^{new} = \beta^{old} - \left(\frac{\partial^2 l(\beta)}{\partial \beta \partial \beta^T}\right)^2 \frac{\partial l(\beta)}{\partial \beta}$$
  
  donde las derivadas se evalúan en $\beta^{old}$. Escribimos nuestras ecuaciones en forma matricial, de forma que $\textbf{y}$ es el vector que contiene a las $y_i$, $\textbf{X}$, de $N \times (p+1)$ la matriz de $x_i$, $\textbf{p}$   el vector de probabilidades $p(x_i;\beta^{old})$ y $\textbf{W}$ una matriz diagonal $N \times N$ con los pesos $p(x_i;\beta^{old})(1-p(x_i;\beta^{old}))$. Así, tenemos
  
  $$\frac{\partial l(\beta)}{\partial \beta} = X^T(y-p)$$
  $$\frac{\partial^2 l(\beta)}{\partial \beta \partial \beta^T} = -X^TWX$$
  
  y por tanto, los pasos del método de Newton son
  
  $$\beta^{new} = \beta^{old} + (X^TWX)^{-1}X^T(y-p)$$
  $$ = (X^TWX)^{-1} X^T W(X \beta^{old}+W^{-1}(y-p))$$
  $$ = (X^TWX)^{-1} X^T Wz$$
  
  Parece que $\beta = 0$ es un buen punto de inicio para el proceso iterativo, aunque la convergencia nunca está garantizada. Generalmente el algoritmo converge, ya que la \textit{log-verosimilitud} es cóncava.
  

  
\newpage
\section{Bibliografía}

%------------------------------------------------

\bibliography{bibliografia} %archivo citas.bib que contiene las entradas 
\bibliographystyle{unsrt} % hay varias formas de citar

\end{document}


%----------------------------------------------------------------------------------------
%	ANEXOS
%----------------------------------------------------------------------------------------

\appendix
\clearpage
\addappheadtotoc
\appendixpage



\end{document} 


