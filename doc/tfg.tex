\input{preambuloSimple.tex}
\graphicspath{ {./images/} }
\usepackage{subcaption}
\usepackage{hyperref}
\usepackage{soul}
\usepackage{amssymb}


%----------------------------------------------------------------------------------------
%	TÍTULO Y DATOS DEL ALUMNO
%----------------------------------------------------------------------------------------

\title{	
	\normalfont \normalsize 
	\textsc{\textbf{Aprendizaje Automático (2019)} \\ Doble Grado en Ingeniería Informática y Matemáticas \\ Universidad de Granada} \\ [25pt] % Your university, school and/or department name(s)
	\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
	\huge Memoria Práctica 3 \\ % The assignment title
	\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Luis Balderas Ruiz \\ \texttt{luisbalderas@correo.ugr.es}} 
% Nombre y apellidos 


\date{\normalsize\today} % Incluye la fecha actual

%----------------------------------------------------------------------------------------
% DOCUMENTO
%----------------------------------------------------------------------------------------

\begin{document}
	
	\maketitle % Muestra el Título
	
	\newpage %inserta un salto de página
	
	\tableofcontents % para generar el índice de contenidos
	
	\listoffigures
	
	\listoftables
	
	\newpage




\section{Contexto: Parkinson y Definición del Problema}

En la actualidad, las enfermedades neurodegenerativas son una de las afecciones más preocupantes para el ser humano y, como tal, es uno de los campos de investigación más importantes que existen. Según \textit{Parkinson's Foundation \cite{pf}}, 46.8 millones de personas en todo el mundo conviven con algún tipo de demencia. Estudios anteriores preveían que, en el año 2020, 42.3 millones de ciudadanos estarían afectados por estas enfermedades. Sin embargo, la ratio de enfermos se ha superado en más de 4 millones un año antes de la fecha esperada, lo que genera una preocupación acuciante. El mismo estudio pronosticó que el número de pacientes con demencia se duplicará en los próximos 20 años. \\

La demencia, a grandes rasgos, es un estado caracterizado por el deterioro de las funciones cerebrales. Este deterioro o pérdida de facultades da lugar a grandes inconvenientes en el día a día, llegando a extremos tan graves como la pérdida de la consciencia. Se estima que hay más de 10 millones de personas enfermos de Parkinson (en lo que sigue, PD) alrededor del mundo (\cite{wp}). Este hecho hace que la investigación de esta enfermedad en concreto sea muy relevante, dado que un diagnóstico precoz podría frenar el desarrollo de la misma. Desgraciadamente, actualmente no existe una cura para el Parkinson, pero sí hay medicamentos que inhiben su desarrollo, dándoles a los pacientes un mínimo de calidad de vida durante un periodo de tiempo más extenso. \\

Los principales métodos de diagnóstico se fundamentan en resultados clínicos, basados en la evaluación médica a través de distintas pruebas al paciente. El diagnóstico actual recae en la presencia de anomalías o disfunciones motoras, signo de que el paciente sufre indudablemente un PD en estado avanzado. En dicho estado, la terapia neuroprotectora apenas produce mejorías sustanciales en los pacientes, por lo que es verdaderamente importante encontrar biomarcadores objetivos y válidos que ayuden a distinguir entre pacientes enfermos de PD de la población sana. \\

En las dos décadas anteriores se adoptaron diversas medidas para el diagnóstico diferencial de PD, incluyendo pruebas olfativas, electrofisiológicas y neuropsicológicas \cite{pruebas-ant}. Sin embargo, neuroimagen es el área más desarrollada para enfrentar diagnósticos. Estos métodos incluye la Imagen de Resonancia Magnética (MRI). MRI es una tecnología no invasiva con una gran resolución espacio-temporal y ha sido enormemente utilizado para el estudio de disfunciones cerebrales de todo tipo. La gran cantidad de información que MRI nos da sobre los tejidos ha mejorado de forma muy sustancial el diagnóstico de patologías cerebrales y su tratamiento. Es conveniente señalar que la basta cantidad de información que nos da está lejos de poder ser procesada manualmente, por lo que urge el desarrollo de herramientas de análisis automatizado. Dicha necesidad hace nacer este proyecto, basado en la extracción de características de imágenes cerebrales para la clasificación de sujetos en enfermos de PD o grupo de control con la mayor exactitud posible.

\subsection{Objetivos del proyecto}

El objetivo principal de este proyecto es diseñar y desarrollar un sistema avanzado que clasifique pacientes en enfermos y sanos tras analizar y refinar datos provenientes de MRI, así como descubrir qué zonas del cerebro son las más determinantes en el diagnóstico de la enfermedad. \\

Existen multitud de artículos en la literatura que llevan a cabo clasificación de pacientes enfermos y sanos. Dicha clasificación de enfermos de Alzheimer o Parkinson suelen estar basadas en la evaluación de las capacidades motoras de los individuos. Sin embargo, este enfoque sobre Parkinson utilizando MRI es novedoso. De la misma manera, utilizaremos métodos de extracción y selección de características, en particular transformada Wavelets 2D y PCA (Análisis de Componentes Principales). Este enfoque ha sido previamente sugerido por otros investigadores (\cite{aggarwal}, \cite{iman}, \cite{deepa}, \cite{mohd}, \cite{rajesh}, \cite{michel}, \cite{jing}, \cite{yudong}, \cite{irojas}, \cite{alberto}). \\

Finalmente, la mayoría de las investigaciones exploran las regiones identificadas por expertos médicos. En este proyecto, nuestro interés es encontrar los planos más relevantes para la clasificación de enfermos de PD. Para ello, existen muchas técnicas de optimización disponible: Optimización por Colonia de Hormigas, Algoritmo de Búsqueda Gravitacional, algoritmo genético NSGA-II. En mi caso, enfoco el problema de una manera totalmente diferente utilizando un ensemble learner basado en Stacking, donde en las primeras capas utilizo SVM con GridSearch para configurar los hiperparámetros y en la segunda, regresión logística (en búsqueda de interpretabilidad).

\subsection{Experimentos}

En la realización del proyecto utilizamos diferentes algoritmos y métodos, incluyendo preprocesamiento de imágenes, extracción, selección de características, clasificación y optimización de los resultados. Los experimentos designados son los siguientes:

\begin{itemize}
	\item El primer experimento fue determinar qué plano de una imagen MRI es más importante en la clasificación de los sujetos. Las imágenes MRI tienen tres planes: X (axial), Y (coronal) y Z (sagittal). Para reducir el tiempo computacional, seleccionamos el plano con los cortes más interesantes.
	
	\item En el segundo experimento, elegido ya el plano correspondiente, comparo el rendimiento de la materia gris, materia blanca y el materia completa para ahorrar costes y mejorar la clasificación.
	
	\item Para la extracción de características, utilizo la transformada discreta Wavelet en 2D.
	
\end{itemize}


\section{Enfermedad de Parkinson y sus estados}

\subsection{Contexto global}

\textit{Parkinson's Foundation} (\cite{pf}) describe el Parkison como sigue: \\

`` El Parkinson es un desorden neurodegenerativo que causa la muerte de las neuronas dopaminérgicas (neurotrasmisores que producen y secretan dopamina) de un área concreta del cerebro llamada \textit{substantia nigra pars compacta (SNpc)}.''

Esta estructura se encuentra localizada en el mesencéfalo y debe su color y su nombre ala presencia de un pigmento llamado neuromelanina que se encuentra dentro de las neuronas que lo forman. 

\begin{figure}[H]
	\begin{minipage}[b]{0.5\linewidth}
		\centering
		\includegraphics[width=\linewidth]{sub-n.png}
		\caption{Susbtantia nigra}
		\label{fig:subs-nig}
	\end{minipage}
	\hspace{0.5cm}
	\begin{minipage}[b]{0.5\linewidth}
		\centering
		\includegraphics[width=\linewidth]{mesenc.png}
		\caption{Mesencéfalo}
		\label{fig:mesenc}
	\end{minipage}
\end{figure}



Estas neuronas dopaminérgicas tienen principalmente la función de regular la actividad motora por medio de la síntesis y la secreción de dopamina, por lo que cuando mueren se manifiestan los típicos signos de la enfermedad que nos resultan familiares: temblores, lentitud en el movimiento (bradiquinesia), inestabilidad, caídas frecuentes... A nivel macroscópico esto se manifiesta en la pérdida de pigmentación característica de la SNpc.

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.4]{pigme.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Pérdida de la pigmentación} 
	\label{fig:pigmentacion}
\end{figure}

Cabe destacar que en las neuronas supervivientes, a nivel microscópico se observan los característicos cuerpos de Lewy, que son unas ``bolsitas'' de proteínas que se acumulan en el citoplasma o cuerpo de la célula.

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.3]{cito.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Cuerpos de Lewy en el citoplasma} 
	\label{fig:lewy}
\end{figure}

\subsection{Corteza cerebral y ganglio basal}

La primera capa que nos encontramos al explorar el cerebro humano es la materia dura, esto es, una membrana que envuelve al cerebro, siendo la última capa de las meninges, cubriendo y por tanto protegiendo el cerebro y la médula espinal. Bajo esta membrana encontramos el cortex, formado por millones de neuronas de un color gris claro (la materia gris) organizadas en seis capas de entre dos y cuatro milímetros de grosor. La corteza cerebral juega un papel trascendental en la conciencia, el pensamiento, el lenguaje, la memoria, la percepción y la atención. La materia gris es una componete muy importante de nuestro sistema nervioso central. Por otra parte, la matria blanca está formada por axones que interconectan las neuronas en diferentes regiones de la corteza y del sistema nervioso central. \\

La corteza cerebral se divide en cuatro lóbulos: \\

\begin{itemize}
	\item Lóbulo temporal: Clave en la percepción auditiva, comprensión del lenguaje, memoria y aprendizaje. Contiene el hipocampo.
	\item Lóbulo frontal: Corteza motora primaria, contiene también la mayoría de las neuronas dopaminérgicas en el cortex.
	\item Lóbulo pariental: esencial para la visión espacial, la navegación y el sentido del tacto.
	\item Lóbulo occipital: Cortex viaul primario, responsable de la creación de los sueños.
\end{itemize}

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.6]{bl.jpg}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Lóbulos de la corteza cerebral} 
	\label{fig:bl}
\end{figure}

Las neuronas dopaminérgicas de la SNpc proyectan sus axones hacia el ganglio basal, formando así el sistema dopaminérgico nigroestriatal. El ganglio basal, que se encuentra en estrecha relación con la SNpc, está descrito como la estructura cerebral más afectada por PD. Cumple un papel esencial tanto en la ejecución de movimientos voluntarios como en actividades cognitivas, por lo que su deterior asociado a PD afectará a estas funciones. \\

El ganglio basal puede verse afectado según el subtipo de la enfermedad. Hay algunos enfermos que sufren cambios microestructurales en la substantia nigra mientras que en otros apenas se aprecia. En general, el ganglio basal acaba por atrofiarse. 

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.6]{gb.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Ganglio basal} 
	\label{fig:gb}
\end{figure}

Según \cite{wp}, los pacientes de PD muestran anisotropía fraccional reducida en la substantia nigra y aumento de la difusividad media y radial en la substantia nigra y el globo pálido (parte del ganglio basal), cuyos efectos pueden verse en técnicas de imagen tales como tractografías.

\subsection{Estadios del Parkinson}

La enfermedad de Parkinson afecta al ser humano de muy distintas maneras. Los enfermos no tienen por qué sufrir los mismos síntomas y, si lo hicieran, tampoco tienen por qué experimentarlos en el mismo orden ni con la misma intensidad. Sin embargo, existen algunos patrones típicos en el progreso de la enfermedad divididos en estadios \cite{wp}:

\begin{itemize}
	\item Estadio uno: Durante este estado inicial, la persona tiene síntomas menores que no interfieren en su vida diaria. Pueden darse temblores y movimientos involuntarios en un lado del cuerpo. De igual manera, se producen cambios posturales, en la forma de andar y en la expresión facial.
	\item Estadio dos: Los síntomas empeoran. Aparecen temblores, rigidez y movimientos involuntarios en ambos lados del cuerpo.
	\item Estadio tres: Considerado el estadio medio, se caracteriza por la ralentización de los movimientos y la pérdida del equilibrio. Las caídas empiezan a ser comunes.
	\item Estadio cuatro: En este punto, los síntomas son severos. Es posible permanecer de pie sin ayuda, pero en general se necesita un andador para desplazarse. La persona es incapaz de vivir sola y requiere asistencia.
	\item Estadio cinco: Este es el estadio más avanzado. Es imposible andar o ponerse de pie por la debilidad en las piernas. La persona requiere silla de ruedas y asistencia total para todas las actividades.
\end{itemize}

\subsection{Consecuencias en el cerebro}

Como hemos comentado, PD afecta a la substantia nigra. También reduce diferentes regiones de la materia gris en el lóbulo temporal. En la figura 2.7 podemos ver una resonancia magnética de un sujeto sano mientras que en la figura 2.8 vemos a un enfermo. Se puede observar la reducción de la materia gris en cada plano  e incluso su desaparición en algunas zonas.

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.35]{healthy.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{MRI de un sujeto sano} 
	\label{fig:healthy}
\end{figure}

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.34]{unhealthy.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{MRI de un sujeto enfermo de PD. Véase la pérdida de materia gris} 
	\label{fig:unhealthy}
\end{figure}

\section{Conceptos y herramientas matemáticas}

En la presente sección discutimos los diferentes conceptos y herramientas utilizadas en este proyecto. Más allá de utilizar la multitud de técnicas que existen dentro de la ciencia de datos aplicada a la biomedicina, mi intención es profundizar desde un punto de visto matemático sus fundamentos y las garantías de su uso. Primero, introduzco la imagen de resonancia magnética (MRI) y la normalización aplicada a las imágenes para los distintos experimentos. A continuación, trataré el concepto de característica en ciencia de datos y me detendré en las herramientas utilizadas para la extracción (transformada discreta Wavelets 2D) y selección (PCA) de las mismas. Por último, abordamos el tema de la clasificación, explicando el algoritmo SVM y las optimizaciones propuestas para la mejora del rendimiento,  interpretabilidad y validez de la investigación.

\subsection{Resonancia magnética nuclear}

Antes de trabajar con imágenes MRI, es necesario tener conocimiento sobre cómo se obtienen, así que introducimos los conceptos más importantes de la resonancia magnética nuclear (NMR \cite{nmr}). Se trata de un método de imagen no invasivo que tiene como objetivo obtener información sobre la estructura y composición de un material. Es muy utilizado en medicina para recabar información sobre tejidos y observar alteraciones o degradaciones de los mismos. \\

NMR es un fenómeno físico basado en las propiedades cuántico-mecánicas del núcleo atómico. Primero, el \textit{spin} (momento angular intrínseco \cite{spin}) de una partícula es un vector asociado al momento magnético de la misma. Tiene una dirección (el eje del \textit{spin}) y un sentido. Cuando dos o más partículas tienen \textit{spin} opuestos están pareadas, la suma de sus momentos es cero y, por tanto, no se produce manifestación alguna del \textit{spin}. Este es el estado natural de los momentos magnéticos en el cuerpo ya que el núcleo de los átomos y sus respectivos electrones están pareados. Sin embargo, podemos encontrar en el cuerpo isótopos con \textit{spin} distinto de cero, siendo los más comunes los de hidrógeno ya que la mayoría de los tejidos contienen agua. La manipulación del \textit{spin} es lo que permite a la máquina de resonancia magnética encontrar las diferencias entre las orientaciones y construir una imagen. Usando campos magnéticos, los núcleos de hidrógeno se alinean magnéticamente, produciéndose este cambio en las alineaciones en un tiempo T1. En presencia de un campo magnético externo, existen dos tipos de orientaciones para el \textit{spin} nuclear:

\begin{itemize}
	\item Paralela, en la que el sentido del momento magnético es el mismo para la partícula y para el campo magnético externo.
	\item Anti-paralelo, cuando ambos dos tienen sentido contrario.
\end{itemize}

Como se muestra en la Figura 3.1, la suma de los momentos magnéticos de un grupo de núcleos de hidrógeno puede ser representado como un vector M paralelo al campo magnético externo $B_0$, donde las componentes normales de los diferentes \textit{spin} se cancelan mutuamente. La mayoría de los \textit{spin} adoptan la orientación paralela, por lo que la suma de los \textit{spins} de M es paralela a $B_0$. \\

M puede ser manipulado usando señales de radio frecuencia. Cuando se aplican, los núcleos absorben la energía y una porción de ella se emite más tarde. Esa es la señal que se detecta. Dependiendo de la intensidad de los campos magnéticos, se pueden obtener imagénes con distintas resoluciones.

 \begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
 	\centering
 	\includegraphics[scale=0.34]{nmr.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
 	\caption{Funcionamiento de los campos magnéticos, spin y NMR} 
 	\label{fig:nmr}
 \end{figure}

\subsection{Normalización}

Antes de manipular o extraer información, es fundamental normalizar el conjunto de datos completo para poder comparar las imágenes. Cada imagen MRI debería representar el mismo espacio, i.e., cada región de la imagen debe describir la misma región del cerebro. Los ventrículos cerebrales deben estar situados en las mismas coordenadas para cada imagen de cada paciente. \\

Dado que los cerebros pueden variar mucho entre pacientes, este objetivo es difícil de conseguir ya que las herencia genética y la vida de cada paciente haga que cada uno tenga sus peculiaridades. Por tanto, debemos tener en cuenta dos ideas a cumplir:

\begin{itemize}
	\item Normalizar la escala de grises de las imágenes para que todas estén en la misma escala.
	\item Redimensionar las imágenes para que todas tengan las mismas medidas.
\end{itemize}

Para completar estas dos tareas, nos remitimos a una plantilla estándar. Para la normalización, utilizamos Statistical Parametric Mapping (SPM \cite{spm}), que es un algoritmo que analiza cada \textit{voxel} usando un test estadístico estándar. Lo utilizamos dada su gran efectividad y su librería de Matlab (a pesar de que el resto del proyecto se realiza en Python). Los valores de los \textit{voxel} son distribuidos de acuerdo a una función de densidad, que suele provenir de las distribuciones T de Student o F. En la Figura 3.2 podemos ver un ejemplo de una imagen antes y después de la normalización.

 \begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.6]{norm.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{A la izquierda, imagen MRI sin normalizar. A la derecha, resultado de la normalización} 
	\label{fig:norm}
\end{figure}

\subsection{Características}

Una de las partes más importantes de un proyecto de inteligencia artificial es la extracción de características. Una característica puede ser entendida como una componente de un vector, que representaría los datos. Cada elemento o individuo del dataset sería representado por un vector n-dimensional (una matriz) en la que cada componente sería una característica. Esta forma de representación es muy intuitiva y flexible ya que permite la introducción y aplicación de conceptos y herramientas matemáticas. \\

El dataset completo sería definido, una vez que cada elemento está representado por sus características, como un conjunto de vectores de $m$ componentes

$$F_i = [f_1,f_2,\dots,f_m]$$

En la expresión, $F_i$ sería un único vector (un paciente) y $f_i$ sus características. Por tanto, si tenemos un grupo de pacientes:

$$F =  \begin{pmatrix}
F_{1} \\
F_{2} \\
\vdots \\
F_{n}
\end{pmatrix} = \begin{pmatrix}
f_{11}& f_{12}& \dots& f_{1m} \\
f_{21}&f_{22}&\dots&f_{2m} \\
\vdots& \vdots& \ddots& \vdots \\
f_{n1}&f_{n2}&\dots&f_{nm}
\end{pmatrix}$$

\subsubsection{Extracción de características}

Cuando se trabaja con MRI, se tiene un número de imágenes de $157\times189\times136$ en los planos X,Y y Z respectivamente, por lo que tenemos 157*189*136 = 4.035.528 \textit{voxels} con 255 valores posibles. De esta manera encontraríamos más de mil millones de características. Necesitamos, por tanto, reducir el espacio de posibilidades, es decir, disminuir la cantidad de recursos necesaria para describir nuestro conjunto de datos. \\

En general, una buena estrategia para extraer características de una imagen es convertirla a un dominio diferente. Con ese objetivo, introducimos ahora dos opciones distintas: la transformada de Fourier y la transformada Wavelets (que finalmente utilizaremos). Todo el material aquí recogido proviene de multitud de textos. Son los siguientes: \cite{math-image}, \cite{canada}, \cite{daube}, \cite{castro}, \cite{misiti}.

\begin{itemize}
	\item \textbf{La Transformada de Fourier en $L^1(\mathbb{R}^d)$}
	
	Dado que la transformada de Fourier es, de forma natural, una función que toma valores en los complejos, asumimos que
	$$u: \mathbb{R}^d \mapsto \mathbb{C}$$
	
	\textbf{Definición 1.} Sea $u \in L^1(\mathbb{R}^d)$ y $\xi \in \mathbb{R}^d$. Se define la transformada de Fourier de $u$ en $\xi$ como 
	
	$$(F u)(\zeta) = \hat{u}(\xi) = \frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^d} u(x) e^{-ix\xi}dx$$


Además, podemos introducir uno de los teoremas más importantes de la transformada de Fourier: el teorema de Convolución: 

\textbf{Teorema de Convolución.} Para $u, v \in L^1(\mathbb{R}^d)$,

$$ F(u*v) = (2\pi)^{d/2}F(u)F(v)$$

\textbf{Demostración.}


Aplicando el teorema de Fubini, obtenemos

$$F(u*v)(\xi) = \frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} u(y) v(x-y) dy\text{ } e^{-ix\xi} dx = $$
$$ == \frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} u(y)e^{-iy\xi}v(x-y)e^{-i(x-y)\xi}dxdy$$
$$ = \int_{\mathbb{R}^d} u(y) e^{-iy\xi} dy F(v)(\xi)$$
$$ = (2\pi)^{d/2}F(u)F(v)(\xi)$$

\hfill$\blacksquare$

A pesar de este resultado, nos será de gran utilidad extender el concepto de transformada de Fourier al espacio de Hilbert $L^2(\mathbb{R}^d)$.

\item \textbf{La Transformada de Fourier en $L^2(\mathbb{R}^d)$}

La extensión de la transformada de Fourier al espacio $L^2(\mathbb{R}^d)$ requiere un poco más de esfuerzo. En primer lugar, definimos un ``pequeño'' espacio de funciones donde la transformada exhibe ciertas propiedades interesantes: el espacio de Schwartz:

\textbf{Definición 2.} El espacio de Schwartz se define como 

$$S(\mathbb{R}^d) = \{u \in C^\infty(\mathbb{R}) | \forall \alpha,\beta \in \mathbb{N}^d: C_{\alpha,\beta}(u) = sup_{x \in \mathbb{R}^d} |x^\alpha \frac{\partial^\beta}{\partial x^\beta}u(x)| < \infty \}$$

Una función $u \in S(\mathbb{R}^d)$ se le llama una función de Schwartz.\\

\textit{Grosso modo}, el espacio de Schwartz contiene funciones suaves que tienden a cero más rápido que los polinomios a infinito. Puede verificarse de forma elemental que el espacio de Schwartz es un espacio vectorial. Con el objetivo de hacerlo accesible por métodos analíticos, lo dotamos de una topología. Describimos dicha topología definiendo una noción de \\convergencia para sucesiones de funciones.

\textbf{Definición 3.} Una sucesión de funciones de Schwartz ${u_n}$ converge a $u$ si y sólo si, para todo multi-índice $\alpha, \beta$, se tiene que 

$$C_{\alpha,\beta}(u_n - u) \mapsto 0 \text{    cuando } n \mapsto \infty$$

La convergencia en el espacio de Schwartz es muy restrictiva: una sucesión de funciones converge si ella y todas sus derivadas multiplicadas por monomios arbitrarios convergen uniformemente. 

Los siguientes lemas serán necesarios en el desarrollo del tema:

\textbf{Lema 1.} El espacio de Schwartz es no vacío y cerrado con respecto a la derivación de cualquier orden y la multiplicación usual.

\textbf{Demostración.} Para $u \in S(\mathbb{R}^d)$, para todo multi-índice $\gamma$, tenemos 

$$C_{\alpha,\beta}(\frac{\partial^\gamma}{\partial x^\gamma}u) = C_{\alpha,\beta+\gamma}(u) < \infty$$

y por tanto $\frac{\partial^\gamma}{\partial x^\gamma}u \in S(\mathbb{R}^d)$. \\

El hecho de que dadas $u, v \in S(\mathbb{R}^d)$, $uv \in S(\mathbb{R}^d)$ puede probarse vía la regla de Leibniz para multi-índices.

\hfill$\blacksquare$

El espacio de Schwartz está muy relacionado con la transformada de Fourier. El siguiente lema presenta reglas de cálculo para transformadas de Fourier sobre funciones de Schwartz.

\textbf{Lema 2.} Sea $u \in S(\mathbb{R}^d), \alpha \in \mathbb{N}^d$ un multi-índice. Se define $p^\alpha(x) = x^\alpha$. Entonces

$$F(\frac{\partial^\alpha u}{\partial x^\alpha}) = i^{|\alpha|}p^\alpha F(u)$$

$$F(p^\alpha u) = i^{|\alpha|} \frac{\partial^\alpha }{\partial x^\alpha} F(u)$$

\textbf{Demostración.}
	Comenzamos con los siguientes cálculos auxiliares:
	
	$$\frac{\partial^\alpha }{\partial x^\alpha} (e^{-ix\xi}) = (-i)^{|\alpha|} \xi^\alpha e^{-ix\xi}$$
	
	$$x^\alpha e{ix\xi} = i^{|\alpha|}\frac{\partial^\alpha}{\partial \xi^\alpha}(e^{-ix\xi})$$
	
	Aplicando integración por partes, obtenemos
	
	$$F(\frac{\partial^\alpha}{\partial x^\alpha}u)(\xi) = \frac{1}{(2\pi)^{d/2}}\int_{\mathbb{R}^d} \frac{\partial^\alpha}{\partial x^\alpha} u(x) e^{-ix\xi}dx$$
	$$= \frac{1}{(2\pi)^{d/2}} i^{|\alpha|}\xi^\alpha \int_{\mathbb{R}^d} u(x) e^{-ix\xi}dx$$
	$$=i^{|\alpha|} p^\alpha Fu(\xi)$$
	
Intercambiando el orden de la integración y la derivación, llegamos a

$$F(p^\alpha u)(\xi) = \frac{1}{(2\pi)^{d/2}}\int_{\mathbb{R}^d} u(x)x^\alpha e^{-ix\xi}dx$$
$$=\frac{1}{(2\pi)^{d/2}}\int_{\mathbb{R}^d} u(x)\frac{\partial^\alpha}{\partial \xi^\alpha}e^{-ix\xi}dx$$
$$=i^{|\alpha|}(\frac{\partial^\alpha}{\partial \xi^\alpha}Fu)(\xi)$$

Los argumentos anteriores son válidos dado que los integrandos son infinitamente derivables respecto a $\xi$ e integrables respecto a $x$.

\hfill$\blacksquare$

Observamos que la transformada de Fourier lleva diferenciación a multiplicación y viceversa. Esto nos lleva a que el espacio de Schwartz va a sí mismo vía la transformada de Fourier. Veámoslo:\\

\textbf{Lema 3.} Para la función $G(x) = e^{-\frac{|x|^2}{2}}$, se tiene que 

$$\hat{G}(\xi) = G(\xi)$$

es decir, la función Gaussiana es una función propia de la transformada de Fourier correspondiente al valor propio uno.

\textbf{Demostración}

La función Gaussiana se puede escribir como un producto tensorial de Gaussianas unidimensionales $g:\mathbb{R} \mapsto \mathbb{R}, g(t) = exp(-t^2/2)$ de forma que $G(x) = \prod_{k=1}^{d} g(x_k)$. Por el teorema de Fubini,

$$\hat{G}(\xi) = \frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^d} \prod_{k=1}^{d} g(x_k) e^{-i x_k \xi_k} dx = \prod_{k=1}^{d} \hat{g}(\xi_k)$$

Para calcular la transformada de Fourier de $g$, tengamos en cuenta que $g$ satisface la ecuación diferencial $g'(t) = -t g(t)$. Aplicando la transformada de Fourier en esa ecuación, por el Lema 2 obtenemos $-\omega \hat{g}(\omega) = \hat{g}'(\omega)$. De hecho, $\hat{g}(0) = \int_{\mathbb{R}}g(t) dt = 1 = g(0)$. Por tanto, las funciones $g,\hat{g}$ satisfacen la misma ecuación diferencial con el mismo valor inicial. Por el teorema de unicidad de soluciones en problemas de valores iniciales de Picard-Lindelöf, $g = \hat{g}$.

\hfill$\blacksquare$

\textbf{Teorema 2.} La transformada de Fourier es una aplicación continua y biyectiva del espacio de Schwartz en sí mismo. Para $u \in S(\mathbb{R}^d)$, tenemos la fórmula de inversión

	$$(F^{-1}Fu)(x) = \overline{\hat{u}}(x)= \frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^d} \hat{u}(\xi) e^{ix\xi} d\xi = u(x)$$

\textbf{Demostración.} Por el lema 2, tenemos que para todo $\xi \in \mathbb{R}^d$

$$|\xi^\alpha \frac{\partial^\beta}{\partial \xi^\beta}\hat{u}(\xi)| = |F(\frac{\partial^\alpha}{\partial x^\alpha}p^\beta u)(\xi)| \leq \frac{1}{(2\pi)^{d/2}} \left\Vert \frac{\partial^\alpha}{\partial x^\alpha}p^\beta u \right \Vert_1  (*)$$

Por tanto, para $u \in S(\mathbb{R}^d)$, tenemos que $\hat{u} \in S(\mathbb{R}^d)$. Dado que la transformada de Fourier es lineal, es suficiente demostrar la continuidad en el cero. En efecto, consideramos a sucesión nula ${u_n}$ en el espacio de Schwartz, es decir, cuando $n \mapsto \infty, C_{\alpha,\beta}(u_n) \mapsto 0$. Esto es, ${u_n}$, al igual que ${\partial^\alpha p^\beta u_n}, \forall \alpha, \beta$, converge uniformemente a cero. Esto implica que la parte derecha de $(*)$ tiende a cero. En particular, obtenemos que $C_{\alpha,\beta}(\hat(u_n)) \mapsto 0$, lo que implica que ${\hat{u_n}}$ es una sucesión nula, probando la continuidad. \\

Para probar la fórmula de inversión, consideramos dos funciones arbitrarias $u, \phi \in S(\mathbb{R}^d)$

$$(\hat{\hat{u}}*\phi)(x) = \int_{\mathbb{R}^d} \hat{\hat{u}}(y) \phi(x-y) dy = \int_{\mathbb{R}^d}\hat{u}(y) e^{ixy} \hat{\phi}(-y) dy$$
$$ = \int_{\mathbb{R}^d} u(y) \hat{\hat{\phi}}(-x-y)dy = (u*\hat{\hat{\phi}})(-x)$$

Ahora elegimos $\phi$ para que sea una función Gaussiana reescalada:

$$\phi_{\epsilon}(x) = \epsilon^{-d}(D_{\epsilon^{-1}id}G)(x) = \epsilon^{-d}e^{-\frac{|x|^2}{2\epsilon^2}}$$

De estos cálculos inferimos que $\hat{\phi}_\epsilon = D_{\epsilon\text{ } id}\hat{G}$ y por tanto $\hat{\hat{\phi}}_\epsilon = \epsilon^{-d} D_{\epsilon^{-1}\text{ } id}\hat{\hat{G}}$. Por el lema 3, $\hat{G} = G$ y $\hat{\hat{\phi_{\epsilon}}} = \phi_{\epsilon}$. Dado que $u$ es en particular acotada y continua, y G es positiva con integral normalizada a uno, podemos aplicar las propiedades de la convolución y obtener que cuando $\epsilon \mapsto 0$,

$$\hat{\hat{u}}*\phi_{\epsilon}(x) \mapsto \hat{\hat{u}}(x)  \text{ y } u*\phi_{\epsilon}(-x) \mapsto u(-x) \Rightarrow \hat{\hat{u}}(x)=u(-x) $$

Por propiedades de conjugación de la transformada de Fourier, tenemos que $\overline{u} = D_{-id} \hat{u}$ y sustituyendo $\hat{u}$ por $u$, obtenemos:

$$\overline{\hat{u}} = D_{-id} \hat{\hat{u}} = u$$


\hfill$\blacksquare$

\textbf{Teorema 3.} Hay un único operador $F:L^2(\mathbb{R}^d) \mapsto L^2(\mathbb{R}^d)$ que extiende la transformada de Fourier $F$ a $S(\mathbb{R}^d)$ y satisface la ecuación $\left\Vert u \right \Vert_2 = \left\Vert Fu \right \Vert_2 \forall u \in L^2(\mathbb{R}^d)$. Además, $F$ es biyectivo y su inversa $F^{-1}$ es una extensión continua de $F^{-1}$ en $S(\mathbb{R}^d)$

\textbf{Demostración.} Para $u,v \in S(\mathbb{R}^d)$, sabemos que 

$$(\hat{u},\hat{v})_2 = (u,v)_2$$

y en particular $\left \Vert u \right \Vert_2 = \left \Vert Fu \right \Vert_2$. Por tanto, la transformada de Fourier es una isometría definida en un subconjunto  denso de $L^2(\mathbb{R}^d)$. Existe una única extensión continua en todo el espacio. Debido a la simetría entre $F$ y $F^{-1}$, un argumento análogo demuestra el recíproco.

\hfill$\blacksquare$

La propiedad de isometría $\left \Vert u \right \Vert_2 = \left \Vert Fu \right \Vert_2$ también implica que

$$(u,v)_2 = (Fu,Fv)_2$$
que se conoce como la fórmula de Plancherel.

\item \textbf{Transformada Wavelet}

En la sección anterior tratamos los fundamentos teóricos que sostienen la transformada de Fourier como herramienta para estudiar la representación de la frecuencia de una señal o imagen. Sin embargo, la información relacionada con la localización no es codificada de una forma plausible. En particular, una alteración local de una señal o una imagen da lugar a una modificación global de toda la transformada de Fourier. En otras palabras, la transformada de Fourier es una transformación global en el sentido de que $\hat{u(\xi)}$ depende de todos los valores de $u$. En ciertas circunstancias, las transformaciones locales son deseables. Antes de introducir nuestra herramienta fundamental en la extracción de características, la transformada Wavelet, presento una nueva alternativa para estudiar información ``local'' en la frecuencia: la transformada de Fourier de tiempo reducido o transformada ``ventana'' de Fourier (\cite{stft}):

\textbf{Definición 4.} Sean $u,g \in L^2(\mathbb{R}^d)$. La transformada de Fourier de tiempo reducido de $u$ con una función ventana $g$ se define como

$$(G_g u)(\xi,t) = \frac{1}{(2\pi)^{d/2}}\int_{\mathbb{R}^d} u(x) \overline{g(x-t)} e^{-ix\xi}dx$$

Esta transformada depende de un parámetro de frecuencia $\xi$ y de un parámetro espacial $t$ y existen muchas formas de representarla:

$$(G_g u)(\xi,t) = F(uT_{-t} \overline{g})(\xi)$$
$$ = \frac{1}{(2\pi)^{d/2}} (u, M_\xi T_{-t}g)_2$$
$$ =  \frac{1}{(2\pi)^{d/2}} (M_{-\xi}u*D_{-id}\overline{g})(t)$$

La primera alternativa explica el nombre de ``ventana'': a través de la multiplicación por la función $g$, $u$ es localizada previa la transformada de Fourier. Nótese que la transformada ``ventana'' de Fourier es una función de $2d$ variables: $(G_g u): \mathbb{R}^{2d} \mapsto \mathbb{C}$. \\

\textbf{Lema 4.} Sean $u,v,g \in L^2(\mathbb{R}^{2d})$. Entonces  $(G_g u) \in L^2(\mathbb{R}^{2d})$ y 

$$(G_g u, G_g v)_{L^2(\mathbb{R}^{2d})} = \Vert g \Vert_2^2(u,v)_2$$

\textbf{Demostración.} Para probar la igualdad entre productos escalares, usamos la isometría de la transformada de Fourier y la fórmula de Plancherel. Con $F_t$ denotamos la transformada de Fourier respecto de $t$, utilizamos una de las alternativas de representación de la transformada ventana de Fourier y el teorema de convolución para obtener

$$F_t(G_g u(\xi,·))(\omega) = F_t((2\pi)^{d/2}(M_{-\xi}u*D_{-id}\overline{g}))(\omega)$$
$$= F(M_{-\xi}u)(\omega)(F D_{-id}\overline{g})(\omega)$$
$$= \hat{u}(\omega+\xi)\overline{\hat{g}}(\omega)$$

Obtenemos el resultado con el siguiente cálculo:

$$(G_g u,G_g v)_{L^2(\mathbb{R}^{2d})} = (F_t(G_g u), F_t(G_g v))_{L^2(\mathbb{R}^{2d})}$$
$$ = \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} \hat{u}(\omega + \xi) \overline{\hat{g}}(\omega) \overline{\hat{v}}(\omega+\xi)\hat{g}(\omega) d\xi d\omega$$
$$= \int_{\mathbb{R}^d} |\hat{g}(\omega)|^2 \int_{\mathbb{R}^d} \hat{u}(\omega+\xi) \overline{\hat{v}}(\omega+\xi) d\xi d\omega$$
$$=\Vert \hat{g} \Vert_2^2(\hat{u},\hat{v})_2$$
$$=\Vert \hat{g} \Vert_2^2(u,v)_2$$

\hfill$\blacksquare$


Vemos entonces que la transformada de Fourier de tiempo reducido es una isometría, por lo que podemos calcular la fórmula de inversión:

\textbf{Corolario 1.} Para $u,g \in L^2(\mathbb{R}^d)$ con $\Vert g \Vert_2 = 1$, tenemos que la fórmula de inversión es

$$u(x) = \frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^d} \int_{\mathbb{R}^d} G_g u(\xi,t) g(x-t) e^{ix\xi} d\xi dt \text{    para casi todo x.}$$


\textbf{Demostración.} Dado que $g$ está normalizada, $G_g$ es una isometría, por lo que sólo nos queda calcular el operador adjunto. Para $u \in L^2(\mathbb{R}^d)$ y $F \in L^2(\mathbb{R}^{2d})$, tenemos que

$$ (u, G_g^*F)_{L^2(\mathbb{R}^d)} = (G_g u,F)_{L^2(\mathbb{R}^{2d})}$$
$$ = \int_{\mathbb{R}^{2d}} G_g u(\xi,t) \overline{F(\xi,t)} d\xi dt$$
$$ = \int_{\mathbb{R}^{2d}} \frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^{2d}} u(x) \overline{g(x-t)} e^{-ix \xi}dx \overline{F(\xi,t)} d\xi dt$$
$$ = \int_{\mathbb{R}^d} u(x) \overline{\frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^{2d}} F(\xi,t) e^{ix\xi}g(x-t)d\xi dt} dx,$$

lo que implica

$$G_g^*F(x) = \frac{1}{(2\pi)^{d/2}} \int_{\mathbb{R}^{2d}} F(\xi,t) e^{ix\xi}g(x-t)d\xi dt$$

\hfill$\blacksquare$

La transformada ventana de Fourier no se aplica en procesamiento de imágenes generalmente. Hay varios motivos: en primer lugar, la transformación de una imagen produce una función de cuatro variables. Esto nos lleva a un gran consumo de memoria y perdemos gran parte de la capacidad de visualización. Por otra parte, la discretización de esta transformada no es para nada trivial y no hay un análogo directo para series de Fourier o transformada discreta. Para más información, véase (\cite{ftta}).

\item Transformada Wavelet continua

Nos centramos a partir de ahora en la herramienta principal en cuanto a la extracción de características se refiere. Mientras que la transformada de tiempo reducido de Fourier usa una ventana fija para localizar la función de interés, la transformada wavelet utiliza funciones variando sus anchuras (Figura 3.3). 

 \begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.42]{widths.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Transformada ventana (izquierda) y transformada wavelet (derecha)} 
	\label{fig:widths}
\end{figure}

En el caso de que nos encontremos en dimensión mayor que uno (nuestro caso), la transformada se puede definir de varias formas. Nos ocupamos primero de la transformada unidimensional:

\textbf{Definición 5.} Sean $u, \psi \in L^2(\mathbb{R},\mathbb{R})$. Para $b \in \mathbb{R}, a>0$ la transformada wavelet de $u$ con $\psi$ se define como

$$L_\psi u(a,b) = \int_{\mathbb{R}} u(x) \frac{1}{\sqrt{a}} \psi\left(\frac{x-b}{a}\right) dx$$

La transformada wavelet depende del parámetro espacial $b$ y el parámetro de escala $a$. Como en casos anteriores, podemos representarla de múltiples formas:

$$L_\psi (a,b) = \frac{1}{\sqrt{a}} (u, T_{-b}D_{1/a}\psi)_{L^2(\mathbb{R})}$$
$$=\frac{1}{\sqrt{a}} (u*D_{-1/a}\psi)(b))$$

Como la transformada de Fourier de tiempo reducido, la transformada wavelet posee cierta isometría, sin embargo, lo hace respecto de una medida ponderada. Introduzco el siguiente espacio:

$$L^2 \left( [0,\infty[ \times \mathbb{R}, \frac{da db}{a^2}\right) = \{ F: [0,\infty[ \times \mathbb{R} \mapsto \mathbb{R}: \int_{\mathbb{R}} \int_{\mathbb{R}^+} |F(a,b)|^2 \frac{da db}{a^2} < \infty \} $$

El producto escalar en este espacio es

$$(F,G)_{L^2 \left( [0,\infty[ \times \mathbb{R}, \frac{da db}{a^2}\right)} = \int_{\mathbb{R}} \int_{\mathbb{R}^+} F(a,b) G(a,b) \frac{da db}{a^2}$$

\textbf{Teorema 4.} Sea $u, \psi \in L^2(\mathbb{R})$ con 

$$ 0 < c_\psi = 2\pi \int_{\mathbb{R}^+} \frac{|\hat{\xi}(\xi)|^2}{\xi} d\xi < \infty$$

Entonces 

$$L_\psi : L^2(\mathbb{R}) \mapsto L^2 \left( [0,\infty[ \times \mathbb{R}, \frac{da db}{a^2}\right)$$

es una función lineal y se tiene que

$$(L_\psi u, L_\psi v)_{L^2 \left( [0,\infty[ \times \mathbb{R}, \frac{da db}{a^2}\right)} = c_\psi(u,v)_{L^2\mathbb{R}}$$

\textbf{Demostración.} Usamos el producto escalar de la transformada wavelet, las propiedades de la transformada de Fourier y la fórmula de Plancherel para obtener

$$L_\psi u (a,b) = \frac{1}{\sqrt{a}}(u,T_{-b}D_{1/a}\psi)_{L^2(\mathbb{R})}$$
$$ = \frac{1}{\sqrt{a}} (\hat{u}, F(T_{-b}D_{1/a}\psi))_{L^2(\mathbb{R})}$$
$$ = \frac{1}{\sqrt{a}} (\hat{u},aM_{-b}D_a\hat{\psi})_{L^2(\mathbb{R})}$$
$$=\sqrt{a} \int_{\mathbb{R}} \hat{u}(\psi)e^{ib\psi}\overline{\hat{\psi}(a\xi)} d\xi$$
$$= \sqrt{a 2\pi} F^{-1} (\hat{u}\overline{D_a \hat{\psi}})(b)$$

Ahora, calculamos lo siguiente:

$$(L_\psi u, L_\psi v)_{ L^2 \left( [0,\infty[ \times \mathbb{R}, \frac{da db}{a^2}\right)} = \int_{\mathbb{R}}
\int_{\mathbb{R}^+} L_\psi u(a,b) L_\psi v(a,b) \frac{da db}{a^2}$$
$$= 2\pi \int_{\mathbb{R}^+} \int_{\mathbb{R}} a F^{-1}(\hat{u}\overline{D_a \hat{\psi}})(b)\overline{F^{-1}(\hat{v}\overline{D_a \hat{\psi}})(b)} db \frac{da}{a^2}$$
$$ = 2\pi  \int_{\mathbb{R}^+} \int_{\mathbb{R}}
a \hat{u}(\xi) \overline{\hat{\psi}(a\xi)}\overline{\hat{v}(\xi)\overline{\hat{\psi}(a\xi)}} d\xi \frac{da}{a^2}$$
$$ = 2\pi \int_{\mathbb{R}} \hat{u}(\xi) \overline{\hat{v}(\xi)} \int_{\mathbb{R}^+} \frac{|\hat{\psi}(a\xi)|^2}{a} da d\xi$$

Un cambio de variable y $|\hat{\psi}(-\xi)| = |\hat{\psi}(\xi)|$ lleva 

$$\int_{\mathbb{R}^+} \frac{|\hat{\psi}(a\xi)|^2}{a} da = \int_{\mathbb{R}^+} \frac{|\hat{\psi}(a|\xi|)|^2}{a} da = \int_{\mathbb{R}^+} \frac{|\hat{\psi}(\omega)|^2}{\omega} d\omega = \frac{c_\psi}{2\pi}$$

Aplicando la fórmula de Plancherel obtenemos lo que buscábamos.

\hfill$\blacksquare$

Que $c_\psi < \infty$ asegura que $L_\psi$ es una función continua y $c_\psi > 0$ garantiza la existencia de la inversa de $L_\psi$. \\

\textbf{Definición 6} La condición

$$0 < c_\psi = 2\pi \int_{\mathbb{R}^+} \frac{|\hat{\psi}(\xi)|^2}{\xi} d\xi < \infty$$

se llama la condición de admisibilidad y la función $\psi$ que la satisface se le llama \textit{wavelets}.

La condición de admisibilidad dice que, alrededor de cero, la transformada de Fourier de una wavelet tiende suficientemente rápido a cero, $\hat{\psi}(0) = 0$.

\textbf{Corolario 2} Sea $u, \psi \in L^2(\mathbb{R})$ y $c_\psi = 1$. Entonces

$$u(x) = \int_{\mathbb{R}} \int_{\mathbb{R}^+} L_\psi u(a,b) \frac{1}{\sqrt{a}} \psi\left(\frac{x-b}{a}\right) \frac{da db}{a^2}$$

\textbf{Desmotración.} Calculamos el adjunto de la transformada Wavelet. Para $u \in L^2(\mathbb{R})$, y $F \in L^2 \left( [0,\infty[ \times \mathbb{R}, \frac{da db}{a^2}\right)$,

$$(L_\psi u, F)_{L^2 \left( [0,\infty[ \times \mathbb{R}, \frac{da db}{a^2}\right)} = \int_{\mathbb{R}} \int_{\mathbb{R}^+} \int_{\mathbb{R}} u(x) \frac{1}{\sqrt{a}} \psi\left(\frac{x-b}{a}\right)dx F(a,b) \frac{da db}{a^2}$$
$$ = \int_{\mathbb{R}} u(x) \int_{\mathbb{R}} \int_{\mathbb{R}^+} F(a,b) \frac{1}{\sqrt{a}} \psi\left(\frac{x-b}{a}\right) \frac{da db}{a^2}dx $$

Esto implica que

$$L_\psi^* F(x) = \int_{\mathbb{R}} \int_{\mathbb{R}^+} F(a,b) \frac{1}{\sqrt{a}} \psi\left(\frac{x-b}{a}\right) \frac{da db}{a^2}$$

como queríamos demostrar.

\hfill$\blacksquare$

Presento ahora la wavelet más sencilla:

\textbf{Haar wavelet:} 

$$\psi(x) = \begin{cases}
	1 \  \ \ si \ 0 \leq x < 0.5 \\
	-1 \ \  si \ 0.5 \leq x < 1 \\
	0 \ \ en \ otro \ caso
\end{cases}$$

Es discontinua pero de soporte compacto:

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.42]{haar.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Wavelet Haar} 
	\label{fig:haar}
\end{figure}

\item Transformada discreta Wavelet

La transformada continua Wavelet es una representación redundante. La pregunta que surge es si sería suficiente con conocer la transformada wavelet de una función en un subconjunto de $[0,\infty[ \times \mathbb{R}$. Este es el caso para algunos subconjuntos discretos. Veamos que bajo ciertas condiciones, las funciones

$$\{\psi_{j,k}(x) = 2^{-j/2}\psi(2^{-j}x-k) | j,k \in \mathbb{Z}\}$$

forman una base ortonormal de $L^2(\mathbb{R})$. Para demostrarlo, primero necesitamos introducir ciertos conceptos. El primo de ellos es la noción de serie wavelet y transformada discreta wavelet, a saber, lo conocido como ``análisis multiescala''. \\

\textbf{Definición 7.} Una sucesión $(V_j)_{j \in \mathbb{Z}}$ de subconjuntos cerrados de $L^2(\mathbb{R})$ es llamado un análisis multiescala si satisface las siguientes condiciones:

\begin{itemize}
	\item Invarianza por traslaciones: $$\forall j,k \in \mathbb{Z}, u_j \in V_j \Leftrightarrow T_{2^jk}u \in V_j$$
	\item Inclusión: $$\forall j \in \mathbb{Z}, V_{j+1} \subset V_{j}$$
	\item Escala: $$\forall j \in \mathbb{Z}, u \in V_{j} \Leftrightarrow D_{1/2}u \in V_{j+1}$$
	\item Intersección trivial: $$\cap_{j \in \mathbb{Z}} V_j = \{0\}$$
	\item Completitud: $$\overline{\cup_{j \in \mathbb{Z}} V_j} = L^2(\mathbb{R})$$
	\item Base ortonormal: Existe una función $\phi \in V_0$ tal que las funciones $\{T_k \phi | k \in \mathbb{Z} \}$ forman una base ortonormal de $V_0$
	La función $\phi$ es llamada un generador o una función de escala del análisis multiescala.
\end{itemize}

Hagamos algunas puntualizaciones sobre esta definición. Los espacios $V_j$ son invariantes por traslación respecto de las traslaciones diádicas $2^j$. Además, están encajadas entre sí y se hacen más pequeñas conforme crece $j$. Si notamos $P_{V_j}$ como la proyección ortogonal sobre $V_j$, entonces para todo $u$ 

$$\lim_{j \mapsto \infty} P_{V_j} u = 0, \lim_{j \mapsto -\infty} P_{V_j} u = 0$$

\textbf{Definición 8.} Sea $(V_j)_{j \in \mathbb{Z}}$ un análisis multiescala. Sean $W_j$ los espacios definidos como los complementos ortogonales de $V_j, V_{j-1}$

$$V_{j-1} = V_j \oplus W_j, V_j \perp W_j$$

El espacio $V_j$ es llamado el espacio aproximación a la escala $j$; el espacio $W_j$ es llamado el espacio wavalet a la escala $j$.

La definición de $W_j$ directamente implica

$$V_j = \oplus_{m \in \mathbb{Z}} W_m$$

y debido a la completitud de $V_j$,

$$L^2(\mathbb{R}) =   \oplus_{m \in \mathbb{Z}} W_m$$

Además, tenemos que $P_{V_{j-1}} = P_{V_j} + P_{W_j}$ y por tanto

$$ P_{W_j} = P_{V_{j-1}} - P_{V_j}$$

Podemos ahora representar cada $u \in L^2(\mathbb{R})$ a través de los espacios $V_j, W_j$ 

$$ u = \sum_{j \in \mathbb{Z}}P_{W_j}u = P_{V_m}u + \sum_{j \leq m}P_{W_j}u$$

Estas ecuaciones justifican el nombre de análisis multiescala: los espacios $V_j$ permiten una aproximación sistemática de funciones a diferentes escalas.

\textbf{Teorema 5.} Sea $(V_j)$ un análisis multiescala con generador $\phi$ tal que $\phi$ satisface 

$$\phi(x) = \sum_{j \in \mathbb{Z}} h_k \sqrt{2} \phi(2x-k)$$ 
con una sucesión $(h_k)$. Más aún, sea $\psi \in V_{-1}$ definido como

$$\psi(x) = \sqrt{2} \sum_{k \in \mathbb{Z}} (-1)^k h_{1-k}\phi(2x-k)$$

Entonces:

\begin{enumerate}
	\item El conjunto $\{\psi_{j,k}: k\in \mathbb{Z} \}$ es una base ortonormal de $W_j$.
	\item El conjunto $\{\psi_{j,k}: k\in \mathbb{Z} \}$ es una base ortonormal de $L^2(\mathbb{R})$.
	\item La función $\psi$ es una wavelet con $c_\psi = 2 \log 2$.
\end{enumerate}

\textbf{Demostración.} Primero vemos que para todo $k \in \mathbb{Z}$, tenemos que

$$(\psi, \phi_{k,0}) = 0,$$
$$(\psi, \psi_{k,0}) = \delta_{0,k}$$

La primera ecuación implica que $\psi \perp V_0$, y por tanto tenemos que $\psi \in W_0$. La segunda ecuación implica la ortonormalidad de las traslaciones de $\psi$. \\

Ahora demostramos que el sistema $\{\psi_{k,0} | k \in \mathbb{Z} \}$ es completo en $W_0$. Dado que $V_{-1} = V_0 \oplus W_0$, es equivalente demostrar que el sistema $\{\phi_{k,0} | k \in \mathbb{Z} \}$ es completo en $V_{-1}$. Esto último lo obtenemos viendo que $\phi_{-1,0}$ se puede representar como $\{\phi_{k,0}, \psi_{k,0} | k \in \mathbb{Z} \}$. Para ello, calculamos lo siguiente:

$$\sum_{k \in \mathbb{Z}} | (\phi_{-1,0}, \psi_{k,0})|^2 +  | (\phi_{-1,0}, \phi_{k,0})|^2$$
$$= \sum_{k \in \mathbb{Z}}| \sum_{l \in \mathbb{Z}} h_l (\phi_{-1,0},\phi_{-1,l+2k}) |^2 + | \sum_{l \in \mathbb{Z}}(-1)^l h_{1-l} (\phi_{-1,0},\phi_{-1,l+2k}) |^2$$
$$= \sum_{k \in \mathbb{Z}} h_{-2k}^2 + h_{1+2k}^2 = \sum_{k \in \mathbb{Z}}h_k^2$$

Se puede ver que $\sum_{k \in \mathbb{Z}} h_k^2 = 1$, y debido que $\Vert \phi_{-1,0} \Vert = 1$, se sigue que el sistema $\{\phi_{k,0}, \psi_{k,0} | k \in \mathbb{Z} \}$ es completo en $V_{-1}$.

Para el tercer punto, hago referencia a \cite{meyer}


\hfill$\blacksquare$

Por último, desarrollo la transformada wavelet discreta bidimensional, la herramienta definitiva para nuestro estudio 2D de las MRI cerebrales:

\item La Transformada Wavelet Discreta Bidimensional

Basándonos en una base wavelet ortonormal $\{\psi_{j,k} | j,k \in \mathbb{Z} \}$ de $L^2)(\mathbb{R})$, podemos construir una base ortonormal de $L^2(\mathbb{R}^2)$ a través de todos los productos tensoriales. Las funciones

$$(x_1,x_2) \mapsto \psi_{j_1,k_1}(x_1), \psi_{j_2,k_2}(x_2), \ \ \  j_i,k_i \in \mathbb{Z}$$

forman una base ortonormal de $L^2(\mathbb{R}^2)$.

En el mismo sentido, podemos constituir un análisis multiescala de $L^2(\mathbb{R}^2)$: para un análisis multiescala $(V_j)$ de $L^2(\mathbb{R}^2)$, creamos los espacios

$$V_j^2 = V_j \otimes V_j \subset L^2(\mathbb{R}^2)$$

que están definidos dado que las funciones

$$\Phi_{j,k}: (x_1,x_2) \mapsto \phi_{j,k_1}(x_1) \phi_{j,k_2}(x_2), \vspace{0.5cm} k= (k_1,k_2) \in \mathbb{Z}^2$$

forman una base ortonormal de $V_j^2$. Esta construcción también es llamada un producto tensorial para espacios de Hilbert separables (\cite{weid}). \\

En el caso bidimensional, los espacios wavelet, es decir, los complementos ortogonales de $V_j^2$ en $V_{j-1}^2$ tienen una estructura más compleja. Definimos el espacio wavelet $W_j^2$ 

$$V_{j-1}^2 = V_j^2 \oplus W_j^2$$

donde el superíndice 2 significa en un caso un producto tensorial y en el otro representa un nombre. Por otra parte, $V_{j-1} = V_j \oplus W_j$, de lo que obtenemos

$$V_{j-1}^2 = (V_j \oplus W_j) \otimes (V_j \oplus W_j)$$
$$ = (V_j \otimes V_j) \oplus (V_j \otimes W_j) \oplus (W_j \otimes V_j) \oplus (W_j \otimes W_j)$$

y llamando

$$H_j^2 = V_j \otimes W_j, \hspace{0.2cm} S_j^2 = W_j \otimes V_j, \hspace{0.2cm} D_j^2 = W_j \otimes W_j$$

podemos expresar

$$V_{j-1}^2 = V_j^2 \oplus H_j^2 \oplus S_j^2 \oplus D_j^2$$

Denotando la functión de escala de $(V_j)$ como $\phi$ y la correspondiente wavelet como $\psi$, definimos tres funciones:

$$\psi^1(x_1,x_2) = \phi(x_1) \psi(x_2), \hspace{0.2cm} \psi^2(x_1,x_2) = \psi(x_1) \phi(x_2), \hspace{0.2cm} \psi^3(x_1,x_2) = \psi(x_1) \psi(x_2)$$

Para $m \in {1,2,3}, j \in \mathbb{Z}, k \in \mathbb{Z}^2$, establecemos,

$$\psi_{j,k}^2(x_1,x_2) = 2^{-j} \psi^m(2^{-j}x_1-k_1,2^{-j}x_2-k_2)$$

Se puede probar que las funciones $\{\psi_{j,k}^1 | k \in \mathbb{Z}^2 \}$ forma una base ortonormal de $H_j^2$, las funciones $\{\psi_{j,k}^2 | k \in \mathbb{Z}^2 \}$ constituyen una base de $S_j^2$ y las funciones $\{\psi_{j,k}^3 | k \in \mathbb{Z}^2 \}$, de $D_j^2$. Por tanto

$$\{\psi_{j,k}^m | m = 1,2,3, k\in \mathbb{Z}^2, j \in \mathbb{Z}\}$$

constituye de forma natural una base ortonormal de $L^2(\mathbb{R}^2)$.

Observamos que los espacios wavelet $W_j^2$ están generados por tres wavelets: $\psi^1, \psi^2$ y $\psi^3$ juntas con sus escalas y traslaciones. Los espacios $H_j^2$ contienen los detalles horizontales en la escala $j$ (en la dirección $x_1$) , los espacios $S_j^2$ los detalles verticales (dirección $x_2$) y los espacios $D_j^2$ los detalles en diagonal:

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.5]{wav.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{La transformada wavelet discreta 2D de una imagen a través de la wavelet Haar} 
	\label{fig:wav}
\end{figure}

Este tipo de análisis multiescala bidimensional es fácil de implementar algorítmicamente: basado en la aproximación de los coeficientes $c^j$, calculamos los coeficientes $c^{j+1}$ es una escala mayor además de los tres coeficientes de detalles $d^{1,j+1}, d^{2,j+1}, d^{3,j+1}$. En la práctica, esto se consigue con la concatenación de la descomposición wavelet unidimensional a lo largo de las columnas y las filas:

\begin{figure}[H] %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics[scale=0.5]{esquema.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
	\caption{Esquema de actuación algorítmica para calcular transformada 2D} 
	\label{fig:esquema}
\end{figure}
\end{itemize}

\newpage

\subsubsection{Selección de características}

Una vez que hemos aplicado la transformada discreta wavelet 2D sobre cada una de los slices y hemos configurado nuestro dataset, nos encontramos con un conjunto de datos de enormes dimensiones, repleto de información (gran parte, probablemente, redundante o inútil en nuestro proyecto) que necesita ser estudiada. Esta basta cantidad de datos tiene dos grandes inconvenientes. El primero de ellos es que los algoritmos de aprendizaje no serán capaces de encontrar las características determinantes a la hora de clasificar instancias debido al gran ruido que tenemos (efecto Hughes). Por su parte, el segundo es un problema pragmático: los clasificadores no van a tener un rendimiento adecuado y tardarán demasiado tiempo en ejecutarse. Por tanto, es necesaria una selección de las características más importantes para así garantizar un mejor rendimiento y unas conclusiones más certeras. Existen multitud de técnicas en estadística multivariante para reducir la dimensionalidad. De entre ellas destaca Análisis de Componentes Principales (PCA) y por ello la utilizamos. PCA es una herramienta eficiente para reducir la dimensionalidad de un dataset formado un gran número de variables correladas entre sí. El objetivo es transformar el conjunto de datos en uno nuevo constituido por las variables ordenadas según el grado de importancia o varianza de las mismas. Esta técnica tiene tres efectos:

\begin{itemize}
	\item Ortogonaliza las componentes de los vectores de entrada para que no tengan correlación entre ellas.
	\item Ordena el resultado de las componentes ortogonales de mayor a menor en variación o importancia.
	\item Elimina aquellas componentes que aportan menor variación.
\end{itemize}

Téngase en cuenta que los vectores de entrada deben estar normalizados con media cero y desviación típica uno antes de utilizar el algoritmo. La normalización se realiza tal y como se explicó en la sección anterior. \\

PCA encuentra una función a través de la cual incluye grandes cantidades de información en muestras analizadas. Sin embargo, como las funciones son combinaciones lineales de las características originales, es difícil (por no decir imposible) interpretar las variables resultantes de la transformación. Desde el punto de vista clínico, este hecho hace que el estudio pierda valor médico, aunque es realmente necesario debido al volumen de datos que tenemos. De manera intuitiva, dado un espacio de características n-dimensional, PCA ajusta un elipsoide de la misma dimensión que contiene un porcentaje prefijado de la varianza de los datos y cada eje se convierte en una componente principal, por lo que si un eje es pequeño, su varianza también lo será. \\

Desarrollo ahora el contenido matemático de Análisis de Componentes Principales (\cite{amsa}, \cite{ms}).
\newpage
	
	Un vector aleatorio es un vector cuyas componentes son variables aleatorias. Por tanto, la media del vector es un vector formado por las medias de cada componente. Si $X = (X_1,\dots,X_n)$ es un vector aleatorio y $\mu = (\mu_1,\dots,\mu_n)$ es su vector media, entonces
	
	$$\mu_i = E(X_i) \hspace{0.5cm} i =1, \dots, n$$
	
	Sin embargo, la analogía para la varianza no es tan obvia. Definimos $var(X)$ como una matriz donde, para cada $i,j$, el elemento es $cov(X_i,X_j)$. Esto es lo que se llama la matriz de covarianzas de $X$. Esta matriz es siempre simétrica. \\
	
	Si $X$ e $Y$ son variables aleatorias e $Y$ es una función lineal de $X$, esto es, $Y = aX + b$ para alguna constante $a$ y $b$, entonces
	
	$$E(Y) = a E(X) + b$$
	$$var(Y) = a^2 var(X)$$
	$$\sigma(Y) =  |a| \sigma(X)$$
	
	Si $X,Y$ son vectores aleatorios, entonces decimos que $Y$ es una función lineal (afín) de $X$ cuando $Y = AX +b$, para alguna matriz constante $A$ y algún vector constante $b$. Tenemos que
	
	$$E(Y) = A E(X) + b $$
	$$var(Y) = A var(X) A^T$$
	
	
	Consideremos el caso en que $Y = AX$, siendo $A$ una matriz fila. Por tanto, $AX$ es un escalar. Por convención, $A = a^T$, siendo $a$ un vector columna. De la igualdad anterior tenemos que
	
	$$ 0 \leq var(Y) = a^T var(X) a$$
	
	teniendo en cuenta que la varianza de una variable aleatoria siempre es no negativa. Esta propiedad tiene un nombre. Un matriz simétrica $V$ se dice que es semi-definida positiva si 
	$$a^T V a \geq 0 \hspace{1cm} \forall a$$
	
	Por tanto, la matriz de covarianzas es simétrica y semi-definida positiva. \\
	
	Tratamos ahora la descomposición espectral. Toda matriz simétrica tiene una descomposición espectral
	
	$$A = ODO^T$$
	donde $D$ es diagonal y $O$ una matriz ortogonal. Pensando en $O$ como un cambio de sistema de coordenadas, encontramos una correspondencia con los movimientos rígidos (rotaciones) con ejes coordenados perpendiculares. La descomposición espectral puede utilizarse para determinar si una matriz es semi-definida positiva. 
	
	\textbf{Proposición.} Una matriz diagonal es semi-definida positiva si y solamente si todos sus elementos son no negativos.
	
	\textbf{Demostración} Basta ver que 
	$$a^T D a = \sum_{i} a_i^2 d_ii.$$
	\hfill$\blacksquare$
	
	En general, una matriz simétrica es semi-definida positiva si y solamente si la matriz diagonal de su descomposición espectral es semi-definida positiva, ya que
	
	$$a^T A a = a^T ODO a = b^t D b$$
	
	donde $b= O^Ta$ y $a = Ob$. \\
	
	Tratamos ahora los valores y vectores propios. Multiplicando a la derecha por $O$, tenemos que
	
	$$AO = ODO^TO = OD$$
	
	Si observamos este resultado por columnas $w_i \in O$, tenemos que
	$$ A w_i = \lambda_i w_i$$
	
	donde $\lambda_i$ es el i-ésimo elemento de $D$.
	
	Por tanto, los elementos de $D$ son los valores propios de $A$ y las columnas de $O$, sus correspondientes vectores propios. \\
	
	Una vez introducidos estos sencillos conceptos, nos adentramos en Componentes Principales. Si $X$ es un vector aleatorio de varianza finita, sea 
	$$var(X) = ODO^T$$
	la descomposición espectral de su matriz de covarianzas.
	
	Considero el vector aleatorio $Y = O^T X$. Entonces
	
	$$var(Y) = O^T var(X) O = O^T ODO^T O = D$$
	
	ya que $O$ es ortogonal. Por tanto, $Y$ tiene una matriz de covarianzas diagonal. Por tanto, como los elementos que están fuera de la diagonal de la matriz de covarianzas son las propias covarianzas de las variables, las componentes de $Y$ no están correladas. Y ya que los elementos de la diagonal de la matriz de covarianzas son la varianza de cada variable y los elementos de $D$ son los valores propios de $var(X)$, las varianzas de los componentes de $Y$ son los valores propios de la matriz de covarianzas de $X$. \\
	
	Los elementos de $Y$ son las llamadas componentes principales de $X$. Dado que una matriz ortogonal es invertible, tenemos que $X = OY$. Esto expresa una variable aleatoria cualquiera como una combinación lineal de variables aleatorias independientes. Este proceso de  hacer la descomposición espectral de la matriz de covarianzas de X es el llamado Análisis de Componentes Principales.
	
	PCA se usa como método de reduccion de dimensionalidad. Si tomamos unos pocos componentes principales, obtenemos una explicación simple de la estructura de $X$ a través de unas pocas variables.Deben ordenarse las componentes de $Y$ poniendo en primer lugar las componentes más grandes (los mayores valores propios). Por último, tratamos la varianza explicada. Sea $\Vert · \Vert$ la norma Euclidea de un vector, por lo que
	
	$$ \Vert Y \Vert^2 = Y^T Y = \sum_{i} Y_i^2$$
	
	Como las componentes de $Y$ son independientes, tenemos que 
	
	$$E(\Vert Y - v \Vert^2) = \sum_{i=1}^{n} \lambda_i$$
	
	donde $v = E(Y)$. Dado que una transformación ortogonal es una rotación (isometría), no afecta a distancias ni longitudes. Si $\mu = E(X)$, tenemos
	
	$$E(\Vert X -\mu \Vert^2) = E((X-\mu)^T (X-\mu)$$
	$$ = E((Y-v)^T O^T (Y-v)$$
	$$ = E((Y-v)^T (Y-v)) $$
	$$E(\Vert Y-v \Vert^2)$$
	
	Dado que $Y = O^TX$, $X = OY$ y $\mu = O v$. De forma similar, si $\overline{\mu} = E(\overline{X})$, entonces
	
	$$E(\Vert \overline{X}- \overline{\mu} \Vert^2) = \sum_{i=1}^{k} \lambda_i$$
	
	La fracción de varianza de $X$ explicada por las primeras $k$ componentes principales es
	$$\frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{n} \lambda_i}$$
	

En nuestro proyecto, utilizamos siempre PCA con una varianza explicada del 95\%.

\newpage
\section{Bibliografía}

%------------------------------------------------

\bibliography{bibliografia} %archivo citas.bib que contiene las entradas 
\bibliographystyle{unsrt} % hay varias formas de citar

\end{document}


%----------------------------------------------------------------------------------------
%	ANEXOS
%----------------------------------------------------------------------------------------

\appendix
\clearpage
\addappheadtotoc
\appendixpage



\end{document} 


